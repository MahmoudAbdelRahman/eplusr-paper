---
title: "eplusr: A framework for integrating building energy simulation and data-driven analytics"
author:
  - name: Hongyuan Jia
    email: hongyuan.jia@bears-berkeley.sg
    affiliation: SinBerBEST
  - name: Adrian Chong
    email: adrian.chong@nus.edu.sg
    affiliation: NUS
    footnote: 1
address:
  - code: SinBerBEST
    address: |
      SinBerBEST Program, Berkeley Education Alliance for Research in Singapore,
      Singapore, 138602, Singapore
  - code: NUS
    address: |
      Department of Building, School of Design and Environment, National
      University of Singapore, 4 Architecture Drive, Singapore, 117566,
      Singapore
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  Building energy simulation (BES) has been widely adopted for the
  investigation of environmental and energy performance for different design
  and retrofit alternatives. In BES, data-driven analytics is of commensurate
  importance in order to turn BES results into understanding, insight, and
  knowledge. However, there is no widely adopted solution to provide seamless
  integration of BES and data-driven analytics. This paper presents a newly
  developed framework 'eplusr' for conducting parametric analysis using
  EnergyPlus via the R programming language. With a data-centric design
  philosophy, the proposed framework focuses on better and more seamless
  integration between BES and data-driven analytics. It provides a Tidy data
  format for BES that matches the semantics of the simulation results which can
  be easily fed to various data analytics workflows in R. The framework also
  provides an infrastructure to bring portable and reusable computation
  environment for building energy modeling using, which aims to facilitate the
  reproducibility research in building energy domain. This paper discusses the
  philosophy behind the framework, its architecture and core capabilities, and
  demonstrates the streamlined workflow of using this framework to conduct data
  exploration, parametric simulation, multi-objective optimization, and
  Bayesian calibration.
journal: "Automation in Construction"
date: "`r Sys.Date()`"
bibliography: references.bib
layout: 3p, times
colorlinks: true
link-citations: true
linenumbers: true
output:
  bookdown::pdf_book:
    includes:
      in_header: header.tex
    base_format: rticles::elsevier_article
    keep_tex: yes
---

```{r setup, include = FALSE}
library(eplusr)
library(kableExtra)
library(here)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  eval = FALSE,
  comment = "#>",
  out.width = "\\columnwidth",
  fig.path = "../figures/",
  fig.pos = "ht"
)

# code chunk cross-ref
Chunk <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function (x, options) {
    x <- Chunk(x, options)
    if (is.null(options$label)) return(x)
    if (!options$echo) return(x)
    if (!is.null(options$fig.cap)) return(x)
    # use chunk label as cross-ref label
    x <- paste0(
        # use Pandoc raw-attribute to preserve raw LaTeX content
        # https://pandoc.org/MANUAL.html#extension-raw_attribute
        "`\\begin{Shaded}`{=latex}\n",
        x, "\n",
        "\\captionof{code}{", options$code.cap, "\\label{code:", options$label, "}}\n",
        "`\\end{Shaded}`{=latex}\n"
    )
    x
})

# set to TRUE to run simulation code chunks
run_sim <- FALSE
```

# Highlights {.unnumbered}

1. A framework integrates EnergyPlus with data science through R
2. The framework includes a parametric simulation  with flexibility and extensibility
3. Tidy data format is introduced to facilitate data-driven analytics of BES research
4. Docker together with R project management can be adopted to bring reproducible BESs

# Introduction

Building energy simulation (BES) is increasingly being used for the analysis and
prediction of building energy consumption, measurement and verification,
life-cycle carbon evaluation and cost analysis of energy conservation measures
(ECMs) [@Chong2017; @Kneifel2010]. It has played a growing role in the design
and operation of low energy, high-performance buildings and development of
policies that drive the achievement of reducing energy use and greenhouse-gas
emissions in the buildings sector [@Hong2018].

BES offers an alternative approach that encourages customized, integrated design
solutions and the development of BES tools has been pronounced over the decades
[@Hong2000; @Hong2018]. The core tools in the BES field are the whole-building
energy simulation programs which provide users with key building performance
indicators such as energy use and demand, temperature, humidity, and costs
[@Crawley2008a]. Among them, EnergyPlus is one of the widely used whole
building energy simulation tools [@Crawley2001] and has been adopted as a core
component or a building simulation engine of energy performance assessment in
both free and commercial building design and analysis tools [@See2011; @DesignBuilderSoftwareLtd2020; @Guglielmetti2011; @Yi2020; @Roudsari2013].

Parametric analysis in BES has been a powerful approach to enable investigation
of environmental and energy performance for different design and retrofit
alternatives. Choosing the appropriate combination of design options is
a complex task that requires the management of a large amount of information
on the properties of design options and the simulation of their performance
[@Purup2020]. Parametric analysis involves tedious file management tasks,
repeated entry of model parameters, the application of design transformations
and the execution of large-scale analyses [@Macumber2012], which can be
time-consuming and error-prone. Parametric simulation task automation has been approved to
be an extremely useful way to reduce human intervention and improve the
efficiency of large parametric analysis. Multiple efforts have been made with
this regard and there are several existing free EnergyPlus-based parametric
simulation frameworks developed, as shown in Table \@ref(tab:pat-sum). Some frameworks
consist of graphical user interfaces (GUIs) built on EnergyPlus while some use
general-purpose scripting languages with a full complement of programming
features and libraries [@Roth2018].

\setlength{\tabcolsep}{0.1pt}
\renewcommand{\arraystretch}{1.3}
```{r pat-sum, eval = TRUE, include = TRUE, echo = FALSE}
# TODO: Add references for mentioned frameworks after changing the bibliography
#       style to numeric
compare <- data.table::fread(here::here("data/compare.csv"))

# use tick symbol
compare[, c(names(compare)) := lapply(.SD, function (val) {val[val == "X"] <- "\\cmark";val})]

# columns that should be further described in footnotes
cols <- c("Semantic API", "Support optimization", "Data extraction", "Weather data handling", "Post-processing capabilities")
data.table::setnames(compare, cols, paste0(cols, kableExtra::footnote_marker_number(seq_along(cols), "latex")))

compare %>%
    .[Name == "Ladybug & Honeybee",  Name := "Ladybug \\& Honeybee"] %>%
    knitr::kable(booktabs = TRUE, format = "latex", linesep = "", escape = FALSE,
        align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
        caption = "Comparison of parametric simulation frameworks based on EnergyPlus") %>%
    kableExtra::kable_styling(font_size = 7, latex_options = "hold_position") %>%
    # have to manually specify the column width since 'full_width' cannot be used here
    # ref: https://github.com/haozhu233/kableExtra/issues/39
    # name
    kableExtra::column_spec(1, "2.8cm") %>%
    # cross-platform
    kableExtra::column_spec(2, "1.2cm") %>%
    # GUI
    kableExtra::column_spec(3, "0.5cm") %>%
    # Open-source
    kableExtra::column_spec(4, "1.1cm") %>%
    # Semantic API
    kableExtra::column_spec(5, "1.1cm") %>%
    # API language
    kableExtra::column_spec(6, "1.3cm") %>%
    # Support optimization
    kableExtra::column_spec(7, "1.6cm") %>%
    # Support calibration
    kableExtra::column_spec(8, "1.6cm") %>%
    # Data extraction
    kableExtra::column_spec(9, "1.4cm") %>%
    # SQL-based structural output
    kableExtra::column_spec(10, "1.5cm") %>%
    # Weather data handling
    kableExtra::column_spec(11, "1.3cm") %>%
    # Post-process capabilities
    kableExtra::column_spec(12, "1.6cm") %>%
    kableExtra::add_footnote(threeparttable = TRUE, notation = "number", c(
        # Semantic API
        "Further abstraction classes to directly perform geometry transformations, HVAC system manipulation and etc.",
        # Support optimization
        "Only built-in features are considered. So as for calibration support. Some frameworks can be further coupled with other software or libraries to perform optimizations",
        # Data extraction
        "The capabilities of extracting further customized summary data, instead of solely based on EnergyPlus built-in functionalities",
        # Weather data handling
        "The capabilities of extracting and modifying data from weather files",
        # Post-processing capabilities
        "The capabilities of performing further data analyses on the extracted simulation results. 'H', 'M' and 'L' indicates 'High', 'Medium' and 'Low', respectively"
    )) %>%
    # change footnote font size manually
    gsub("\\small", "\\scriptsize", x = ., fixed = TRUE)
```

OpenStudio [@Guglielmetti2011] is a free open-source software toolkit designed
for energy modeling and can be used to efficiently create or modify models,
manage individual or multiple simulations, and visualize results. OpenStudio
has its own format (`.OSM`) and schema for EnergyPlus model representation
which will eventually be translated into EnergyPlus models. Parametric Analysis
Tools (PAT) is a GUI application that is part of OpenStudio toolkit. It aims to
enable customizable and shareable parametric descriptions of ECMs
[@Parker2014]. It leverages OpenStudio Measures which are reusable scripts
written in Ruby programming language to manipulate OpenStudio models and can be used to compare
manually specified combinations of measures, optimize designs, calibrate models
and perform parametric sensitivity analysis. Ladybug and Honeybee are plug-ins
developed for Rhino Grasshopper [@Roudsari2013]. Ladybug is capable to import
and analyze EnergyPlus standard weather data (EPW) in Grasshopper, while
Honeybee can create, run and visualize the OpenStudio and EnergyPlus simulation
results. Both Ladybug and Honeybee take full advantage of the visual scripting
interfaces provided by Grasshopper [@Tabadkani2019]. Since the primary input
files of EnergyPlus are all ASCII text-based, it is possible to update building
energy models by directly processing and manipulating the text files, without taking
into account the complex hierarchical structure in the model components. There
are several applications that take this approach. jEplus [@Yi2020] is a
software written in Java programming language to perform complex parametric analysis on multiple
design parameters. It allows users to describe the parameters and their values
using customized symbols in a graphical user interface, and automatically create
parametric models using text-substitution [@Zhang2010a]. Modelkit
[@BigLadderSoftware2020] is another free and open-source framework for parametric
model using EnergyPlus. Compared to jEplus, it is capable of automating the
generation and management of EnergyPlus models via its templates and scripting
tools written in Ruby. MLE+ integrates EnergyPlus and scientific computation
and design capacities of Matlab for controller design and can be used to
implement and simulate advanced control algorithms of building systems
[@Bernal2012; @Zhao2013b]. eppy [@Philip2020] is a library for manipulating
EnergyPlus models programmatically via python programming language. It parses
EnergyPlus IDF files into a python object and provides low-level programmatic
access to EnergyPlus inputs. EpXL [@Schild2020] is an EnergyPlus
Microsoft Excel user-interface written in Visual Basic for Applications (VBA)
that enables import and export of IDF data files, parametric analysis and
optimization. It is capable of displaying a compact tabular overview of input
data and automatically importing simulation output into Excel, with a link for
viewing the 3D model. GenOpt is a generic optimization programme that can be
used with EnergyPlus to minimize an objective function with respect to multiple
parameters [@Wetter2001]. It generates new input templates files by replacing
the keywords with corresponding numerical values calculated using a
mathematical optimization library. The frameworks mentioned above may have
overlapping in features but they are tailored for different purposes and use
cases, with the primary focus on ease the time-consuming and error-prone
process of creating and managing parametric simulations.

Data-driven analytics are complementary with BES in nature [@Srivastava2019]
aiming to build systems and algorithms to discover knowledge, detect patterns,
and generate useful insights and predictions from BES data [@Molina-Solana2017;
@BurakGunay2019]. It encompasses the whole data science process, beginning from
data extraction and cleaning, and extending to data analysis, description and
summarization. BES, with an iterative nature inside, can produce a large amount
of data. The volumes of BES data have clearly overwhelmed traditional data
analysis methods such as spreadsheets and ad-hoc queries with large amount of
factors to be considered [@Kim2011]. Moreover, the output of common BES tools
is not always friendly in format for applying these methods which makes data
pre-processing an essential but time-consuming and laborious process for any
data-driven analytics for BES data. This highlights the potential areas for
improvements in data extraction and result presentation in a clear and
intuitive manner for data analytics. Even some BES tools provide summary
reports with various details, it is still quite common in BES to perform
post-processing and apply customized and more advanced algorithms to the
simulation results. Currently, there is a growing body of scientific literature
on the application of advanced mathematical algorithms for building design
using BES [@Kiss2020]. Researches on parametric building energy simulations
that employ data mining algorithms upon large amounts of simulations have
emerged [@BurakGunay2019]. Solutions in most existing frameworks have limited
post-processing capacities and are not flexible enough to enable a clear
understanding and control on how the BES data is being transformed
[@Miller2013; @Attia2013a]. Open-source programming environments such as R
[@RCoreTeam2019] and Python [@Oliphant2007] are promising on providing
solutions for large-scale data analytics and have become widely-used research
tools that provide access to many well-documented packages for various data
mining, machine learning and data visualization applications [@Lowndes2017;
@Molina-Solana2017]. However, fewer efforts have been made in terms of
providing a seamless integrated approach to bridge the gap between BES and
data-driven analytics. Results of a survey of 448 building energy
management professionals in the U.S. show that there is a need of improving the
efficacy and integration of data-driven analytics and BES in the building
energy management domain, and efforts should be made to develop integrated
tools that are capable of leveraging both methods [@Srivastava2019].

BES is a complex domain that involves multiple scientific processes and is
often a time-intensive, error-prone and hard-to-reproducible process.
Reproducibility is defined as the ability to recompute data analytic results,
given an observed data set and knowledge of the data analysis pipeline
[@Peng2015]. Reproducible research has received an increasing level of
attention throughout the scientific community and the public at large
[@Boettiger2015]. According to a Nature's survey of 1576 researchers in 2016,
more than 70% of them failed to reproduce another scientist's experiments, and
more than 50% failed to reproduce their own experiments. Moreover, more than
half of them agreed that there was a significant crisis of reproducibility
[@Baker2016a]. As BES becomes more and more integral to many aspects of architecture design,
and decision-making processes, computational reproducibility has become an
issue of increasing importance to researchers, designers and practitioners.
Lack of credibility in BES results due to a lack quality and reproducibility is
widely considered a problem by the energy model community [@Fleming2012].
Despite the culture of reluctance and a lack of requirements or incentives to
publish the code used in generating the results, the reasons why BES is usually
not easy to reproduce is mainly caused by 2 aspects:

1. *A lack of seamless integration of BES and data-driven analytics workflows*

   When conducting BES, most users prefer to use GUI applications which makes it
   intuitive and easy to execute certain particular tasks. However, GUI tools have
   constraints on flexibility as the users have to specify exactly what and how
   features of the design can be manipulated and often are not be able to provide
   a good workflow for repeating that task across a wider range of situations on
   different systems. In this case, manual steps have to be performed using other
   tools, such as a spreadsheet or command-line tools, which introduces
   additional transcription burden and results in a non-reproducible process
   [@Macumber2012]. Sometimes, custom solutions have to be created from scratch to
   automate part portions of the workflows, which may lead to new inefficiencies
   and potential errors.

   \vspace{5pt}

   One of the goals of BES is to turn raw simulation data into understanding,
   insight, and knowledge. BES projects often encompass an iterative workflow of
   raw design data importing and tidying, parametric input file generation and
   modification, simulation result collection, processing and analyses, and
   finally, conclusion visualization. Most of the time, data analytics of BES
   results is of commensurate importance with the model itself. Currently,
   there is no widely adopted solution that is able to integrate all processes
   into one single platform.

2. *Hard to setup the computational environment which covers the whole BES and data-driven analytics toolchain*

   BES often involves the use of multiple applications, software and platforms.
   To perform crucial scientific processes such as
   replicating the results, extending the approach or testing the conclusions in
   other contexts, the indispensable step is to install the software used by the
   original researchers, which sometimes can become immensely time-consuming if
   not impossible. It is easy to underestimate the significant barriers raised by
   a lack of familiar, intuitive, and widely adopted tools for addressing the
   challenges of computational reproducibility [@Boettiger2015].

In summary, multiple efforts have been made to improve the wide adoption of BES
among researchers, designers and practitioners. However, an overall holistic
framework to bridge the gap between the building energy simulation and data
science domains is still undefined. There is also a need for an integrated
framework to provide a supportive environment to bring reproducible building
energy modeling.

In this paper, we introduce a new framework called 'eplusr' for integrating BES
and data-driven analytics. Eplusr is different from existing frameworks because
of its data-centric design philosophy. It provides a parametric simulation
prototype with a unified data interface ensuring BES data is returned in a
consistent format to match the semantics of the simulation results. This makes
the BES data can be seamlessly integrated into data-driven analytics workflows
with minor processing. The proposed framework also provides infrastructures to
bring portable and reusable computation environment for BES, aiming to
facilitate the reproducibility research in building energy domain. This paper
first introduces the concepts and behind the framework and its modules, along
with its implementation. Next, the applications of the framework using a medium
office building model is presented, covering various topics including data
exploration, parametric simulation, optimization and calibration.

# Methodology

Fig. \@ref(fig:architecture) shows an architecture overview of the eplusr
framework which contains two main components, including a free open-source
library written in R programming language and a computation environment for
reproducible BES encapsulated using Docker containerisation [@Merkel2014]. The
eplusr R package aims to provide better and more seamless integration between
BES engine EnergyPlus and R-programming data-driven analytics environment
through a parametric simulation prototype together with a tidy data
[@Wickham2014] interface. It takes advantages of the EnergyPlus SQLite output
and formats parametric BES output data in a tidy form that matches the
semantics of the simulation results and thus makes it can be easily fed to
various existing data-driven analytics workflows in R. The eplusr R package is
distributed using CRAN (The Comprehensive R Archive Network) with the MIT
license [@Jia2020] and the source code is hosted publicly in a
version-controlled repository via GitHub^[GitHub Repository:
https://github.com/hongyuanjia/eplusr]. The Docker containerisation for BES
aims to provide infrastructure to bring portable and reusable computation
environment to facilitate the reproducibility research in building energy
modeling domain. The pre-built images are publicly available through Docker
Hub^[Docker Hub Link: https://hub.docker.com/r/hongyuanjia/eplusr]. A further
detailed description of the tidy data interface (Section
\@ref(sec:eplusr-tidy)) and the parametric simulation prototype (Section
\@ref(sec:eplusr-parametric)) of the R package, and the concepts and components
of the Docker containerisation for BES (Section \@ref(sec:docker)) is introduced.

```{r architecture, echo = FALSE, eval = TRUE, fig.cap = "Eplusr framework architecture", out.width = "40%", fig.pos = "H"}
knitr::include_graphics(here("figures/overview.png"))
```

## Tidy data interface {#sec:eplusr-tidy}

The tidy data interface is implemented through three modules in the eplusr R
package, including I/O Processing & Parser Module, Model API Module and Tidy
Data Extractor Module. The concept of tidy data format was first proposed by
@Wickham2014, which is a standard way of mapping the meaning of a dataset to
its structure. In tidy data, each variable forms a column, each observation
forms a row and each type of observational unit forms a table. This structure
makes it easy for an analyst or a computer to extract needed variables and is
particularly well suited for vectorized programming languages like R, because
the layout ensures that values of different variables from the same observation
are always paired [@Wickham2014; @Wickham2017] and is always fit for directly
handling by the *tidyverse* ecosystem, which is a language for solving
data-driven analytics challenges using R [@Wickham2019a].

The tidy data interface is the key implementation in this framework to achieve
provide better and more seamless integration between BES and data-driven
analytics. It is th reflection of the data-centric design philosophy of the
framework and is designed to facilitate exploration and analysis of the BES
models and simulation outputs, and to simplify the development of BES analytics
tools that work well together. This interface aims to:

* Provide tidy data representation of BES models and weathers
* Provide BES model modification APIs to take tidy data input
* Provide interfaces to query BES output and return resulting data in tidy format

Each of them will be discussed in details in the following sections.

### Tidy data representation of BES models and weathers

The I/O processing & parser module shown in Fig. \@ref(fig:architecture) aims
to read, parse and represent EnergyPlus models and weathers in relational tidy
format tables, providing the flexibility to extract and modify BES models and
weathers using numbers of structured data toolkits available in R.

EnergyPlus requires a very detailed and data-intensive text input format called
Input Data File (IDF), with its own data format schema named Input Data
Dictionary (IDD). In eplusr, data of an IDF and the corresponding IDD are
stored as Relational Databases (RD) and encapsulated into an `Idf` object and
`Idd` object, respectively. RD was first proposed by @Codd1990 and has become
the dominant database model for a number of Relational Database Management
Systems (RDMS). It organizes data in a set of rectangular tables with rows and
columns. Each table has a primary key which is a unique identifier constructed
from one or more columns and a table is linked to another by including the
other table's primary key which is called a foreign key for that table. This
feature helps keep the data integrity and reduce any data redundancy. Fetching
the required data is achieved by using join queries and conditional statements
to combine all or any number of related tables.

Fig. \@ref(fig:data-structure) shows the relationships among tables in an `Idf`
and `Idd` object with the primary keys presented in light blue color. The IDD
data is stored in four tables of (1) `group`, (2) `class` and (3) `field` to
retain the hierarchy structure of the data schema, and (4) `reference` to store
the referencing relations among various fields. Similarly, the core IDF data is
stored in three tables of (1) `object`, (2) `value`, and (3) `reference` to
keep the referencing relations among various field values. The RD data
structure follows the idea of database normalization where each fact is
expressed in only one place, and enables fast table joining among entities and
variables. To modify an IDF is equal to change the data in `Idf` tables
accordingly, in the context of specific `Idd` data. Based on this data
structure, an extensive rule-based data model validator has been developed in
the library to check the integrity of `Idf` values during parsing and any
modifications.

To support analyses of weather data in BES-based environmentally-informed
design and model calibration process, the I/O processing & parser module
contains a `Epw` class to create, parse, and modify EnergyPlus EPW weather
files. The `Epw` class stores weather metadata and core weather data into a
list of tidy tables that can be easily extracted and fed into time-series
analytics libraries in R.

```{r data-structure, echo = FALSE, eval = TRUE, fig.cap = "Data structure of an Idf and Idd object", out.width = "70%"}
knitr::include_graphics(here("figures/data_structure.png"))
```

### Tidy data input API for BES model modifications

The Model API module shown in Fig. \@ref(fig:architecture) aims to provide
flexible, rich-featured and user-friendly interfaces to perform queries and
modifications on EnergyPlus models programmatically. In total, more than ten
methods have been developed. Each method will validate input data to make sure
any modification result complies with the underlying IDD.

The Model API module provides flexibly-scoped interfaces to modify field values
in a model, including single-object level, grouped-object level, and
whole-class level, enabling to alter large number of objects using few lines of
code. Both `Idf` and `IdfObject` class provide a `to_table()` method to
extract certain or all parts of a model into one tidy `data.table` object,
which is an extension of R's table representation but extremely optimized for
fast computation [@Dowle2019]. This makes it possible to directly adjust energy
models by table data transformation, utilizing advanced algorithms that R
provides. The `load()` and `update()` methods in `Idf` class can take that tidy
data as input, and respectively create and modify models accordingly.

### Tidy data extractor for BES outputs {#sec:eplusr-tidy-extractor}

The tidy data extractor module shown in Fig. \@ref(fig:architecture) aims to
extract and represent EnergyPlus simulation outputs in relational tidy format
tables. Instead of directly reading and manipulating EnergyPlus CSV and HTML
output files, the tidy data extractor module introduces a `EplusSql` class that
utilizes EnergyPlus SQLite output format to extract simulation results. SQLite
is a mature and widely-employed RDMS [@Owens2006]. The main benefits of using
the EnergyPlus SQLite output format are that it contains all of the data in
standard reports, variable output, meter outputs and a number of standard input
and output summaries, and thus makes it possible to build tidy-data interfaces
to retrieve various kinds of simulation results based on a single file.

The `EplusSql` class provides four main methods to extract tidy data, including
`read_table()` for the standard input and output summaries, ,
`report_data_dict()` for the variable and meter dictionary, `report_data()` for
the variable and meter outputs, and `tabular_data()` for the standard reports.
Fig. \@ref(fig:tidy-extractor) shows the schematic of the `report_data()`
method implementation and the other three methods follow similar design
philosophy. The extractor interface provides features to retrieve results of
given variables and meters with certain key identifier, at specified time,
simulation run period, reporting frequency and unit. It achieves this by
sending SQL (Structured Query Language) queries, a domain-specific language for
RDMS to the simulation output SQLite database. The resulting tidy tables will a
result of table joining using four tables, including `Time`,
`EnvironmentPeriods`, `ReportDataDictionary` and `ReportData`, where `Time`
contains the time of simulation, including month, day, hour, day of the week
and etc, `EnvironmentPeriods` contains the names and types of the simulated run
periods, `ReportDataDictionary` table provides the dictionary of reported
variables and meters including key identifiers, variable names, units and
reported frequencies, and `ReportData` contains the actual values of reported
variables. The primary keys are highlighted with light blue color in and fields
that can be specified through the interface are highlighted with light purple
color. \@ref(fig:tidy-extractor).

```{r tidy-extractor, echo = FALSE, eval = TRUE, fig.cap = "Schematic of the tidy data extractor for variable and meter outputs", out.width = "80%"}
knitr::include_graphics(here("figures/result_extraction_interface.png"))
```

However, the result returned by the SQL queries does not contain a complete
time-series data because the `Time` table stores the time of simulation in five
separate columns of `Month`, `Day`, `Hour`, `Minute` and `DayType` (day of the
week), without a year specification^[A `Year` field was added in recent version
of EnergyPlus. But old versions of EnergyPlus are still widely used.]. This makes it
impossible to directly apply to the result with time-series-based algorithms in
R. To solve this issue, a year derivation algorithm is implemented that
calculates a proper year value for each run period based on the five time
columns and compose a complete series of `POSIXct` values, which is the
standard date-time class in R.

## The parametric prototype {#sec:eplusr-parametric}

The parametric prototype provides a set of abstractions to ease the process of
parametric model generation, design alternative evaluation and large parametric
simulation management. It is implemented through two modules in the eplusr R
package, including Simulation Manager Module, and Parametric Prototype Module.

The `ParametricJob` class is the workhorse of the parametric prototype
implementation, as shown in Fig. \@ref(fig:data-structure). It inherits from
the `EplusGroupJob` class in the Simulation Manager Module and is capable of
effectively managing multiple simulation jobs. Fig. \@ref(fig:parametric) shows
the workflow of conducting parametric simulations using this framework.

```{r parametric, echo = FALSE, eval = TRUE, fig.cap = "Workflow of a parametric simulation", out.width = "70%", fig.pos = "H"}
knitr::include_graphics(here("figures/parametric.png"))
```

A `ParametricJob` object is initialized using a seed model and a weather file.
Design alternatives are specified by applying a *measure* function to the seed
model. The concept of *measure* in the prototype is inspired by similar concept
in OpenStudio [@Guglielmetti2011] but tailored for flexibility and
extensibility. A measure is simply an R function that takes an `Idf` object and
any other parameters (e.g. $t_1$ to $t_5$ in Fig. \@ref(fig:parametric)) as
input, and returns a set of modified `Idf` objects as output, making it
possible to leverage all the modules that the eplusr R package provides and any
applicable advanced statistical methods and libraries existing in R. This
implementation enables users to deploy customized applications to parameterize
and formulate the design alternatives. After a measure is defined, the method
`apply_measure()` takes it and other parameter values specified to create a set
of parametric models for later simulations. Calling the `run()` method will
run all parametric simulations in parallel and place each simulation outputs in a
separate folder. All simulation metadata will keep updating during the whole
time and can be retrieved using the `status()` method for further
investigations.

The `ParametricJob` class leverages the tidy data extractor described in
Section \@ref(sec:eplusr-tidy-extractor) to retrieve parametric simulation
results in tidy format. Despite of that, a number of methods are also provided
to read various output files, including errors (`eplusout.err`), report data
dictionary (`eplusout.rdd`) and meter data dictionary (`eplusout.mdd`). For all
result tidy tables, an extra column containing the simulation job identifiers
is prepended in each table and can be used as an index or key for further data
transformations, analyses and visualisation to compare simulated design options.

The proposed parametric prototype is designed to be simple yet flexible and
extensible. One good example of the extensibility of this framework is the
epluspar [@Jia2020a] R package, which provides new classes for conducting
specific parametric analysis on EnergyPlus models, including sensitivity
analysis using Morris method [@Morris1991] and Bayesian calibration using the
method proposed by @Chong2017. All the new classes introduced are based on the
`ParametricJob` class. The main difference mainly lies in the specific
statistical method used for sampling parameter values when calling
`apply_measure()` method. Few examples of this application have been provided
in Section \@ref(sec:applications).

## Computation environment for reproducible BES {#sec:docker}

The Docker containerisation for BES, as shown in Fig. \@ref(fig:architecture),
aims to provide infrastructure to bring portable and reusable computation
environment to facilitate the reproducible BES application. @Peng2015
summarized two major components to a successful reproducible research: (1)
data, i.e. the availability of raw data from the experiment, and (2) code, i.e.
the availability of the statistical code and documentation to reproduce the
results. In the context of BES, these will be (1) the building energy models
and (2) the code to perform simulations and following data-driven analytics.
However, the model and data are only parts of the whole BES computational
environment. The complex and rapidly changing nature of computer environments
makes it immensely challenging to reproduce the same workflow and results even
with the original data and code are available. To solve this issue, a
reproducible BES computation environment has been developed based on the Docker
containerisation technology, enabling to capture the full software stack
including all software dependencies in a portable and reusable images.

Docker [@Merkel2014] is a popular open source tool for containerisation and has
shown its potential to improve computational reproducibility [@Boettiger2015;
@Nust2020]. The Rocker Project was launched in 2014 as a collaboration to
provide high-quality Docker images containing the R environment, and has seen
both considerable uptake in the R community and substantial development and
evolution [@Boettiger2017]. The proposed reproducible BES computation
environment is built upon the `rocker/verse` images and contains four groups of
toolchains needed for common BES and data-driven analytics workflows using the
eplusr framework:

1. Statistical computing environment, including the latest R environment and
   RStudio Server, a web-based integrated development environment for R
   programming
1. BES engine, including EnergyPlus of specified version and the eplusr R
   package
1. Data analytics toolkits, including a collection of tidyverse [@Wickham2019a]
   R packages for data import, tidying, manipulation, visualisation and
   programming
1. Literate programming environment, including R Markdown related R packages
   for dynamic document generation

The first three have been described in previous sections. Literate programming
is a programming paradigm introduced by @Knuth1984 in which the explanation of
a computer program is given, together with snippets of source code. Recently,
there have been significant efforts to develop literate programming
infrastructure to reproducibly perform and communicate data analyses, including
R Markdown [@Grolemund2018] , Jupyter notebook [@Kluyver2016], just to name a
few.

The R Markdown format is underneath powered by the knitr R package [@Xie2015]
and Pandoc [@Krewinkel2017]. Knitr executes the computer code written in
various programming languages embedded, and converts R Markdown to Markdown.
And Pandoc processes the resulting Markdown and render it to various output
format, including PDF, HTML, Word, and etc. The R Markdown format has been a
widely adopted authoring framework for data science. It can be used to both
save and execute code, and generate high-quality reports that can be shared
with an audience. Together with rmarkdown [@Allaire2020] and tinytex [@Xie2019]
packages, the proposed BES computation environment can be easily adapted to any
R-centric workflows and enables researchers in BES field to build and archive
reproducible analytics.

The source files of Docker configuration were written in several text files
so-called Dockerfiles and are publicly available and hosted via GitHub^[GitHub
Repository: https://github.com/hongyuanjia/eplusr-docker]. Thus further
evolutions can be taken to make the computation environment tailored for
different audience and use purposes. The docker approach is particularly well
suited for moving between local and cloud platforms when a web-based integrated
development environment is available, such as RStudio Server [@Boettiger2015],
providing the scalability potential for large cloud-based BES computation.

# Applications {#sec:applications}

To show how the eplusr framework can be used, examples are presented in four
topics: (1) data exploration, (2) parametric simulation, (3) multi-objective
optimization using Genetic Algorithm (GA), and (4) calibration using Bayesian
theory. For all examples, the U.S. Department of Energy (DOE) medium office
reference building model in compliance with Standard ASHRAE 90.1 -- 2004
[@Field2010] is used. Fig. \@ref(fig:medium-office) shows a 3D view of the
building geometry. It represents a 3-sotry, 15-zone office building with total
floor area of 4982 $\mathrm{m}^2$. Each floor was divided into 1 core zone and
4 perimeter zones. Three multi-zone variable air volume (VAV) with reheat coil
systems, with each served by a direct-expansion (DX) cooling coil and a gas
burner, were installed to provide the conditioned thermal environment for each
floor. The typical meteorological year 3 (TMY3) weather data of Chicago was
used for all the simulations.

```{r medium-office, echo = FALSE, eval = TRUE, fig.cap = "3D view of DOE medium office reference building", out.width = "40%"}
knitr::include_graphics(here::here("figures/medium-office.png"))
```

## Data exploration on the EUI and the heating and cooling demands {#sec:basic}

The energy use intensity (EUI) is one key indicator for building energy
performance and its breakdown can provide insightful directions of where ECMs
should be applied to reduce energy usage. When evaluating the feasibility of
free-cooling application in buildings, the heating and cooling demand profile
plays an important role in determination the potential. This example
demonstrates the data exploration process of obtaining these two outcomes from
an annual simulation results. It showcases the basic features of the eplusr
framework with main focus on how the tidy data extractor can provide seamless
workflow to extract BES output, feed it into data analysis pipeline and turn
the results into understanding and knowledge. Listing \@ref(code:basic) shows
the R code to achieve it.

Line 31 -- 50 in Listing \@ref(code:basic) shows how to use methods
`tabular_data()`, `read_table()` and `report_data()` in the `EplusSql` class to
extract building area and building energy consumption from standard reports,
zone metadata from standard input and output, and cooling and heating demands
from variable output, with all formatted in a tidy representation. Table
\@ref(tab:mess-csv) and Table \@ref(tab:tidy-csv) gives an example of
EnergyPlus CSV output and results from tidy data extractor from the eplusr R
package for the same variable output, respectively. Table \@ref(tab:tidy-csv)
is the first 10 rows with selected columns of output from line 46 -- 50 in
Listing \@ref(code:basic).

One problem when working with EnergyPlus CSV output is that the data is not in
a tidy format and a number of information is concatenated in the column
headers, i.e. column headers contain values, not only variable names. In Table
\@ref(tab:mess-csv), one column header was composed in format *Key value*
(`VAV_1`) + *Variable name* (`Air System Total Heating Energy`) + *Units* (`J`) +
*Reporting frequency* (`Hourly`). This often leads to additional data cleaning
efforts. For instance, to get all key values for variable `Air System Total
Heating Energy`, users have to write their methods of splitting column headers
into different parts or subsetting column using regular expressions, which may
lead to new inefficiencies and potential errors. Compared to Table
\@ref(tab:mess-csv), the tidy data extractor represented the same underlying
data in a tidy format, as shown in Table \@ref(tab:tidy-csv). It is equivalent
to transform the original column header into four separate variables of
`key_value`, `name`, `units` and `reporting_frequency`. Moreover, instead of
presenting the simulated date and time as strings in Table \@ref(tab:mess-csv),
a time-series column `datetime` in `POSIXct` class was created based on a
derived year value using the algorithm described in Section
\@ref(sec:eplusr-tidy-extractor). An additional column named `case` was also
added, which is by default the IDF file name without extension. `case` column
can be quite useful and be served in data analytics as an identifier to
separate each model simulation results when extracting outputs from multiple
parametric simulations. Besides columns currently shown in Table
\@ref(tab:tidy-csv), the extractor also provides a number of additional columns
shown in Fig. \@ref(fig:tidy-extractor), which makes it quite convenient and
straightforward to directly perform further data transformations, including row
filtering and grouped summarization, demonstrated in Line 93 -- 107 in Listing
\@ref(code:basic).

```{r mess-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data/mess.csv")), linesep = "",
    caption = "Example of EnergyPlus CSV output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8, latex_options = "hold_position") %>%
    column_spec(1, "2.5cm")
```

\setlength{\tabcolsep}{1.0pt}
```{r tidy-csv, echo = FALSE, eval = TRUE}
tidy <- data.table::fread(here::here("data/tidy.csv"))
knitr::kable(tidy, linesep = "", align = "c",
    caption = "Tidy format of EnergyPlus output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8, latex_options = "hold_position") %>%
    column_spec(1, "2.0cm") %>%
    column_spec(2, "2.6cm") %>%
    column_spec(3, "1.5cm") %>%
    column_spec(4, "4.6cm") %>%
    column_spec(5, "2.9cm") %>%
    column_spec(6, "0.7cm")
```

Based on the building energy consumption data (line 39 in Listing
\@ref(code:basic)) and the building area (line 36 in Listing
\@ref(code:basic)), the electricity EUI breakdown from various end-use
categories was calculated and a pie chart was created (shown in Fig.
\@ref(fig:eui)) using only 13 lines of codes (line 56 -- 77 in Listing
\@ref(code:basic)). From Fig. \@ref(fig:eui), we can see that most of the
energy has been consumed by interior electric equipment, followed by indoor
lighting, indicating ECMs that help to reduce the plug loads and lighting power
density (LPD) may have a promising on improving the overall energy performance.
We will perform further investigations on this in Section \@ref(sec:param).

```{r eui, echo = FALSE, eval = TRUE, fig.cap = "Annual electricity EUI breakdown", out.width = "50%"}
knitr::include_graphics(here::here("figures/eui.png"))
```

Fig. \@ref(fig:aircon-out) shows the profile of monthly heating and cooling
demands in unit $\mathrm{MJ}/\mathrm{m}^2$. It is calculated based on the zone
metadata (line 42 in Listing \@ref(code:basic)) and hourly air system heating
and cooling energy outputs (line 46 -- 50 in Listing \@ref(code:basic)). With
the tidy format and additional metadata columns, these two data fit well in the
data pipeline, making it straightforward and intuitive to perform data
transformation and visualization. Fig. \@ref(fig:aircon-out) is a result of
only around 30 lines of code (line 41 -- 50 and 79 -- 119 in Listing
\@ref(code:basic)). In Fig. \@ref(fig:aircon-out), we can
see that during the transition seasons, including March, April, October and
November, the heating and cooling demands are relatively small compared to
summer and winter seasons, indicating the potential of free-cooling
application.

```{r aircon-out, echo = FALSE, eval = TRUE, fig.cap = "Monthly heating and cooling demand profile", out.width = "50%"}
knitr::include_graphics(here::here("figures/aircon_out.png"))
```

## Parametric simulation of ECMs on plug loads and LPD {#sec:param}

Since plug loads and interior lighting systems consumed more than 60% of total
electricity, as shown in Fig. \@ref(fig:eui), it is worthwhile investigating
the energy saving potentials of ECMs on reducing the plug loads and LPD. This
example demonstrates the process to achieve it by parametric simulations using
the parametric prototype in the eplusr framework. Listing \@ref(code:param)
showcases the workflow of utilizing the model API to build measures and reusing
the data analysis code snippets developed in Section \@ref(sec:basic) directly
on the parametric simulation results.

In the original model, the LPD for office room is 10.76
$\mathrm{W}/\mathrm{m}^2$. Energy savings could be achieved using higher
efficiency lightings, for instance fluorescent T5 (~7 $\mathrm{W}/\mathrm{m}^2$
LPD), and LED (~5 $\mathrm{W}/\mathrm{m}^2$ LPD). Line 4 -- 14 in Listing
\@ref(code:param) shows a simple measure that modifies the LPD. The core code
is line 14 that assigns all related fields in a whole class to input values,
taking advantages of the flexibly-scoped model API.

Fig. \@ref(fig:sch) shows the schedule profile of lights and plug loads during
weekdays. During the off-work time, we can see that there were still at least
40% plug loads running, indicating the potential to implement optimal
control strategies to turn plug loads off at night to provide energy savings,
without impacting the day-to-day operation of the building occupants. Line 20
-- 40 in Listing \@ref(code:param) shows a measure that modifies the off-work
schedule values of plug loads by multiplying a specified reduction faction
value. This measure aims to demonstrate how objects in an energy model can be
translated into tidy data and how to use the tidy data input API to alter the
model.

```{r sch, eval = TRUE, echo = FALSE, fig.cap = "Schedule profile of lights and plug laods during weekdays", out.width = "50%", fig.pos = "H"}
knitr::include_graphics(here::here("figures/sch.png"))
```

Different measures can be chained together and supplied to the
`apply_measure()` method in the parametric prototype to create parametric
models. As demonstrated in line 42 -- 57 in Listing \@ref(code:param), the
combined measure `ecm` is used to create six models with various combinations
of LPD and plug loads control strategies. Each model is given a specific name.
After the parallel run of simulations (line 67 in Listing \@ref(code:param)),
the tidy data extractor is used to read building energy consumptions of all six
models using one line of code (line 67 in Listing \@ref(code:param)). The
resulting data format is the same as that of a single simulation and is
equivalent to bind rows from six tables into one tidy table with the `case`
column working as an identifier by filling it with names specified in line 56
in Listing \@ref(code:param). This data structure makes it effortless to reuse
most of the EUI breakdown calculation code in Listing \@ref(code:basic), and
perform case filtering (line 80 in Listing \@ref(code:param)), table joins
and grouped summarization (line 84 -- 85 in Listing \@ref(code:param)).

Fig. \@ref(fig:savings) shows the energy savings of various lighting
technologies and plug loads control strategies, based on line 82 -- 96 in
Listing \@ref(code:param). All technologies show overall energy savings to
various degrees. Using higher efficiency lightings shows a very promising
savings in both reducing the lighting electricity usage, with T5 and LED saving
34.9%% and 53.5% respectively, and the corresponding overall energy savings for
T5 and LED are 7.5% and 11.4%. Strategies of turning off 40% and 80%
unnecessary plug loads during off-work hours reduce 11.3% and 22.6% electricity
usage from interior equipment and improve the overall energy performance by
3.0% and 5.8%, respectively. Additional energy savings can be obtained when
incorporating LED with 80% reduction factor in off-work plug loads. However,
even the overall energy savings are positive for all cases, the trend for the
heating energy shows the opposite. This is due to the reason that all examined
technologies will reduce indoor heat gains which plays a positive role during
heating seasons.

```{r savings, eval = TRUE, echo = FALSE, fig.cap = "Energy savings of various lighting technologies and plug loads control strategies", out.width = "60%", fig.pos = "H"}
knitr::include_graphics(here::here("figures/savings.png"))
```

## Multi-objective optimization using Genetic Algorithm

Automated optimization has become increasingly popular in BES research and
applications to efficiently search and identify optimal or near optimal design
options meeting one or more key design performance objectives [@Attia2013a].
The epluspar R package implements a `GAOptimJob()` class which is based on the
proposed parametric prototype and the ecr R package for modular framework of
evolutionary computation [@Bossek2017]. It attempts to implement flexible
general-purpose GA interfaces to solve BES-based single- or multi-objective
optimization problems. This example aims to demonstrate the workflow of
performing multi-objective optimization (MOO).

Listing \@ref(code:moo) shows the actual R code for MOO of reducing carbon
emissions and discomfort hours of the medium office reference building at the
same time, by varying indoor heating and cooling setpoint temperatures,
window-to-wall ratio (WWR) and exterior wall insulation thickness. The workflow
shown in Listing \@ref(code:moo) can be divided into four main parts: (1)
create optimization objective functions, (2) specify optimization variables,
(3) set GA operators and (4) gather results and perform further analyses.

In this example, the two main objectives are reducing carbon emissions and
discomfort hours. Line 11 -- 18 in and line 21--27 Listing \@ref(code:moo)
defined functions to extract the annual total carbon emissions and discomfort
hours counted based on the Standard ASHRAE 55 -- 2004 from the standard reports
using tidy data extractor. Line 30 in Listing \@ref(code:moo) took the two
objective functions and told the algorithm the minimization optimization
direction using the `objective()` method.

Line 36 -- 96 in Listing \@ref(code:moo) defined measure functions to accept
various design options in terms of indoor heating and cooling setpoint,
window-to-wall ratio (WWR) and exterior wall insulation thickness. Line 89
--104 specified the type, range or choices of optimization parameter, using the
similar `apply_measure()` interface. Here we varied heating setpoint
temperature from 18 °C to 22 °C, and cooling setpoint temperature from 23 °C to
27 °C. Both modified with a step of 0.5 °C. The WWR and exterior wall
insulation thickness are continuous variables, with lower and upper bound of
(20%, 80%), and (0.02 m, 0.5 m), respectively. The `GAOptimJob` class provides
a `validate()` method for checking all objective and measure functions by
running a sizing or full simulation with randomly generated design options and
evaluating the fitness to make sure no error occurred.

GA has three key genetic operators: recombinator (also called crossover),
mutator, and selector, providing detailed procedures and steps on how to
generate children from parent solutions. Line 114 -- 118 in Listing
\@ref(code:moo) directly specified those three operators with the default
values that the `GAOptimJob` class provided and tweaked to directly perform MOO
using the Non-Dominated Sorting Genetic Algorithm (NSGA-II). The `terminator()`
method was used to specify the conditions to terminate the computation. In this
example, we set it to stop when one hundred generations were evaluated.

With all objectives, variables and operators specified, the optimization
started with twenty individuals per generation, resulting in a total of two
thousands annual energy simulations. Line 126 and 129 in Listing
\@ref(code:moo) extracted all population and Pareto font into two tidy tables
for future analyses, using the `population()` and `pareto_set()` method. Fig.
\@ref(fig:pareto) shows the Pareto front of discomfort hours and total carbon
emissions, generated using line 132 -- 139 in Listing \@ref(code:moo). The
final Pareto font contained 20 unique solutions.

```{r pareto, echo = FALSE, eval = TRUE, fig.cap = "Pareto front of discomfort hours and carbon emissons", out.width = "40%"}
knitr::include_graphics(here::here("figures/pareto.png"))
```

Fig. \@ref(fig:parallel) shows the parallel coordinates charts of the Pareto
set. The carbon emissions have seen a significant reduction from the original
value of 290ton. However, there were 10 out of 20 solutions in the Pareto set
that performed worse in terms of providing satisfactory indoor thermal
environment. One possible solution to avoid this is to add a constraint when
evaluating the fitness of the `discomfort_hours` objective, making sure all
solutions that have larger discomfort hours should be abandoned.

```{r parallel, echo = FALSE, eval = TRUE, fig.cap = "Parallel coordinates chart of the Pareto set", out.width = "70%"}
knitr::include_graphics(here::here("figures/parallel.png"))
```

## Model calibration using Bayesian theory

Model calibration is an essential process to achieve greater confidence in BES
results. Recent years there has been an increasing application of Bayesian
approaches for BES calibration [@Chong2017]. Bayesian calibration is carried
out following the statistical formulation proposed by Kennedy and O'Hagan
[@Chong2018]. This example aims to demonstrate the model calibration workflow
using the epluspar R package. The epluspar R package implements the Bayesian
calibration algorithm proposed by @Chong2017 and guidelines proposed by
@Chong2018, and encapsulates it into the `BayesCalibJob` class, which is based
on the parametric prototype. Listing \@ref(code:bc) shows the workflow of
calibrating one VAV fan total efficiency in the medium office reference model
using observed fan input air flow rate and electricity power.

The initial step for the Bayesian calibration is to collect data for observable
input and output, and weather conditions. Since the reference model represents
a virtual building with no measured data, we created some synthetic data of
examined period July 1st to July 3rd using simulations (line 1 -- 27 in Listing
\@ref(code:bc)). Also, the TMY3 weather data was used, instead of the Actual
Meteorological Year (AMY) weather data. For real practice, the actual
measurable variables may not be directly representable in EnergyPlus. So the
first step may also include a mapping process to transform measured variables
into EnergyPlus output variables and connect the transformed measured values
with the model using schedule files or other techniques.

The next step was to specify the observable input and output variables using
the `input()` and `output()` methods in `BayesCalibJob` class (line 29 -- 39 in
Listing \@ref(code:bc)). The calibration parameters, together with the number
of EnergyPlus simulations was described using the `param()` method (line 41 --
45 in Listing \@ref(code:bc)). Each calibration parameter was given a lower and
upper bound value. Following the Bayesian calibration guidelines [@Chong2018],
the epluspar R package also introduces a `SensitivityJob` class based on the
parametric prototype to perform calibration parameter screening with Morris
method. Once the calibration parameters were given, the `samples()` method in
`BayesCalibJob` class will use the Latin Hypercube Sampling (LHS) algorithm to
generate parameter sample values based on the lower and upper bound and the
simulation number specified (line 47 -- 48 in Listing \@ref(code:bc)). The
benefit of LHS is that it will try to cover as much as possible in the
multi-dimensional space of the calibration parameters. After all simulations
completed (line 50 -- 51 in Listing \@ref(code:bc)), the `data_sim()` method
gathered all data of simulated output variables and aggregated them into the
same time frequency as the actual measured data (line 53 -- 54 in Listing
\@ref(code:bc)). With all data required specified (line 56 -- 60 in Listing
\@ref(code:bc)), the `stan_run()` method was used to employ the Bayesian
calibration algorithm written in probabilistic programming language Stan (line
62 -- 63 in Listing \@ref(code:bc)). Once completed, the posterior
distributions of calibration parameters, predicted output variable values and
uncertainty statistical indicators including Normalized Mean Biased Error
(NMBE) and Coefficient of Variation of the Root Mean Squared Error (CVRMSE) can
be retrieved using method `post_dist()`, `prediction()` and `evaluate()`,
respectively.

Fig. \@ref(fig:dist-post) gives a density plot showing the posterior
distributions of calibrated fan total efficiency, created using line 74 -- 79
in Listing \@ref(code:bc). The mean value of the posterior distribution was
0.60, which is quite close to the actual value of 0.5915 specified in the
original model. Fig. \@ref(fig:uncertainties) gives a box plot showing the
distribution of CVRMSE and NMBE per Markov Chain Monte Carlo (MCMC) sampling,
created using line 81 -- 85 in Listing \@ref(code:bc). The mean NMBE value was
quite close to zero and the averaged CVRMSE is round 3.0%. Both values met the
thresholds of CVRMSE $\leq$ 15% and NMBE $\leq$ 5% set by ASHRAE [@ASHRAE2014].
The satisfactory results are expected, since we use synthetic data. However,
the overall workflow shown in this example can be applied to Bayesian
calibration on building energy models with real measured data.

```{r dist-post, echo = FALSE, eval = TRUE, fig.cap = "Posterior distribution of fan total efficiency", out.width = "40%"}
knitr::include_graphics(here("figures/dist_post.png"))
```

```{r uncertainties, echo = FALSE, eval = TRUE, fig.cap = "Distribution of CVRMSE and NMBE per MCMC sample", out.width = "40%"}
knitr::include_graphics(here("figures/uncertainties.png"))
```

# Conclusion

Building energy simulation (BES) has been widely adopted for investigation of
environmental and energy performance for different design and retrofit
alternatives. The absence of seamless integration of BES and data-centric
analysis raises problems in both the productivity and also the credibility of
BES study. This paper proposed a novel holistic framework called 'eplusr' to
bridge the gap between the building energy simulation and data science domains.

Eplusr differs from existing frameworks by its data-centric design philosophy.
It provides a tidy data interface for BES that matches the semantics of the
simulation results with the data representation. The tidy data extractor
provides the possibilities to query BES outputs with various types of
specifications, which makes it easy and quite straightforward to get simulation
results of any specified time, units and variables in a consistent manner. The
tidy-formatted results can be easily fed to various data-centric analytics
using existing tools in R.

The parametric prototype developed in this framework provides a set of
abstractions to ease the process of parametric model generation, design
alternative evaluation and large parametric simulation management. It is
capable of defining various analyses using any algorithms available in R. The
flexibility and extensibility of the parametric simulation prototype in this
framework is demonstrated by its easy adoption to perform multi-objective
optimization and Bayesian calibration.

The need for reproducibility in BEM is growing significantly together with the
ongoing trend of the increasing complexity of BEM project. The eplusr framework
provides a possible solution for this by developing a portable and reusable BES
computation environment based on Docker containerisation, encapsulating the
toolchains for statistical computing, building energy modeling, data analytics
and literate programming.

# Limitations and future work

The main limitation of the proposed framework lies in its R-oriented workflows,
which currently may not be widely adopted in industry fields. Prospective users
of the framework who do not know R must spend time learning how to use it. This
drawback may be compensated with the growing user community.

Since the eplusr framework is mainly focus on modifying existing BES models,
instead of creating new ones from scratch, currently it has limited capacities
to perform sophisticated geometry transformation including surface matching and
rotation. Operations such like creating or replacing one whole HVAC system may
also be time-consuming processes.

# Acknowledgements {-}

This research was funded by the Republic of Singapore's National Research
Foundation through a grant to the Berkeley Education Alliance for Research in
Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and
Sustainability in the Tropics (SinBerBEST) Program. BEARS has been established
by the University of California, Berkeley as a center for intellectual
excellence in research and education in Singapore.

`\begin{appendices}`{=latex}

\renewcommand{\thesection}{\Alph{section}.}

# Code for data exploration on the EUI and the heating and cooling demands

```{r basic, echo = TRUE, eval = run_sim, code.cap = "Data exploration on the EUI and the heating and cooling demands"}
# load package
library(eplusr)
library(tidyverse) # for data-driven analytics

# get EnergyPlus v9.1 installation directory
dir <- eplus_config(9.1)$dir

# use example model and weather file distributed with EnergyPlus v9.1
path_model <- file.path(dir, "ExampleFiles/RefBldgMediumOfficeNew2004_Chicago.idf")
path_weather <- file.path(dir, "WeatherData/USA_IL_Chicago-OHare.Intl.AP.725300_TMY3.epw")

# read model
idf <- read_idf(path_model)

#############
# Model API #
#############

# make sure weather file input is respected
idf$SimulationControl$Run_Simulation_for_Weather_File_Run_Periods <- "Yes"

# make sure energy consumption is presented in kWh
idf$OutputControl_Table_Style$Unit_Conversion <- "JtoKWH"

# save the modified model into a temporary folder
idf$save(file.path(tempdir(), "MediumOffice.idf"))

# run annual simulation
job <- idf$run(path_weather)

#######################
# Tidy data extractor #
#######################

# read building area from Standard Reports
area <- job$tabular_data(table_name = "Building Area", wide = TRUE)[[1L]]

# read building energy consumption from Standard Reports
end_use <- job$tabular_data(table_name = "End Uses", wide = TRUE)[[1L]]

# read zone metadata from Standard Input and Output
zones <- job$read_table("Zones")

# read hourly air-conditioning system output with all additional metadata for
# the annual simulation from Variable Output
aircon_out <- job$report_data(
    name = sprintf("air system total %s energy", c("heating", "cooling")),
    environment_name = "annual",
    all = TRUE
)

#########################
# Data-driven analytics #
#########################

# calculate Energy Use Intensity (EUI) for electricity
eui <- end_use %>%
    # only select columns of interest
    select(category = row_name, electricity = `Electricity [kWh]`) %>%
    # get rid of category with empty energy consumption
    filter(electricity > 0.0) %>%
    # order by value
    arrange(-electricity) %>%
    # calculate EUI
    mutate(eui = round(electricity / area$'Area [m2]'[1], digits = 2)) %>%
    # calculate proportion of each category
    mutate(proportion = round(eui / eui[1] * 100, digits = 2)) %>%
    # remove electricity column
    select(-electricity)

# plot a pie chart to show EUI breakdown
p_eui <- eui %>%
    filter(category != "Total End Uses") %>%
    mutate(category = as_factor(sprintf("%s [%.2f%%]", category, proportion, "%"))) %>%
    ggplot(aes("", proportion, fill = category)) +
    geom_bar(stat = "identity", width = 1, color = "black", size = 0.2) +
    coord_polar("y", start = 0)

# calculate air-conditioned floor area per storey
storey <- zones %>%
    # exclude plenum zones
    filter(is_part_of_total_area == 1) %>%
    # group by centroid height
    group_by(centroid_height = round(centroid_z, digits = 4)) %>%
    # calculate total floor area
    summarise(floor_area = sum(floor_area)) %>%
    ungroup() %>%
    # add storey index
    arrange(centroid_height) %>%
    mutate(storey = seq_len(n()), air_system = paste("VAV", storey, sep = "_")) %>%
    select(air_system, floor_area)

# get monthly heating and cooling demands per served area
aircon_out_mon <- aircon_out %>%
    # only consider weekdays
    filter(!day_type %in% c("Holiday", "Saturday", "Sunday")) %>%
    # add an identifier column to indicate cooling and heating condition
    mutate(type = case_when(
        str_detect(name, "Heating") ~ "Heating",
        str_detect(name, "Cooling") ~ "Cooling"
    )) %>% 
    # add floor area served by each air-conditioning system
    left_join(storey, c("key_value" = "air_system")) %>%
    # calculate the monthly averaged heating and cooling demands in MJ/m2
    group_by(month, type, air_system = key_value) %>%
    summarise(system_output = sum(value) / 1e6 / floor_area[1]) %>%
    ungroup()

# plot a pie chart to show the heating and cooling demand profile
p_aircon_out <- aircon_out_mon %>%
    mutate(month = as.factor(month)) %>%
    mutate(system_output = case_when(
        type == "Heating" ~ -system_output,
        type == "Cooling" ~ system_output
    )) %>%
    ggplot() +
    geom_col(aes(month, system_output, group = type, fill = type), position = "dodge") +
    facet_wrap(vars(air_system), ncol = 1) +
    labs(x = "", y = "Heating and cooling demands / MJ m-2")
```

```{r basic-post-process, echo = FALSE, eval = run_sim}
tidy <- aircon_out[order(key_value, -name), .(case, datetime, key_value, name, reporting_frequency, units, value)][1:10]
data.table::fwrite(tidy, here("data/tidy.csv"), dateTimeAs = "write.csv")

mess <- job$report_data(
    key_value = "vav_1", name = "air system total heating energy",
    wide = TRUE, month = 1, day = 1, hour = 1:10)[, -"case"]
data.table::fwrite(mess, here("data/mess.csv"))

# colorblind-friendly palette
pal_cb <- c(
    "#999999", #1
    "#E69F00", #2
    "#CC79A7", #3
    "#56B4E9", #4
    "#009E73", #5
    "#F0E442", #6
    "#0072B2", #7
    "#D55E00"  #8
)

p_eui_post <- p_eui +
    theme_void() +
    scale_fill_manual(values = pal_cb) +
    theme(plot.margin = unit(c(-1.0, 0, -1.0, -1.0), "cm"))
ggsave(here("figures/eui.png"), p_eui_post, width = 6, height = 3.5, dpi = 600)

p_aircon_out_post <- p_aircon_out +
    scale_fill_manual(values = pal_cb[c(4, 3)])
ggsave(here("figures/aircon_out.png"), p_aircon_out_post, width = 6, height = 6, dpi = 600)
```

# Code for parametric simulation of ECMs on plug loads and LPD

```{r plot-sch, echo = FALSE, eval = run_sim}
# extract the schedule of plug loads during weekdays
sch_plug <- idf$
    ElectricEquipment$
    Core_bottom_PlugMisc_Equip$
    ref_to_object("Schedule Name")[[1]]$
    to_table()[5:20]

sch_plug <- data.table::data.table(
    hour = sch_plug[1:.N %% 2 == 1, as.integer(str_extract(value, "\\d+"))],
    val  = sch_plug[1:.N %% 2 == 0, as.double(value)])[
    J(1:24), on = "hour", roll = -Inf]

sch_light <- idf$Lights$
    Core_bottom_Lights$
    ref_to_object("Schedule Name")[[1]]$
    to_table()[5:22]

sch_light <- data.table::data.table(
    hour = sch_light[1:.N %% 2 == 1, as.integer(str_extract(value, "\\d+"))],
    val  = sch_light[1:.N %% 2 == 0, as.double(value)])[
    J(1:24), on = "hour", roll = -Inf]

sch <- data.table::rbindlist(list(
    sch_plug[, type := "Plug laods"], sch_light[, type := "Lights"]
))

p_sch <- sch %>%
    ggplot() +
    geom_line(aes(hour, val, color = type), size = 1) +
    scale_x_continuous(NULL, breaks = seq(1, 24, 2), expand = c(0, 0)) +
    scale_y_continuous("Schedule value", breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
    scale_color_manual(values = pal_cb[c(2, 5)]) +
    guides(color = guide_legend(NULL)) +
    coord_cartesian(ylim = c(0, 1)) +
    theme_classic() +
    theme(
        legend.position = "bottom",
        panel.grid.major = element_line(color = "grey95")
    )
ggsave(here::here("figures/sch.png"), p_sch, width = 6, height = 3, dpi = 600)
```

```{r param, echo = TRUE, eval = run_sim, code.cap = "Parametric simulation of ECMs on plug loads and LPD"}
# create a parametric prototype of given model and weather file
param <- param_job(idf, path_weather)

#####################
#  Create Measures  #
#####################

# create a measure for modifying LPD
set_lpd <- function (idf, lpd = NA) {
    # keep the original if applicable
    if (is.na(lpd)) return(idf)

    # set 'Watts per Zone Floor Area' in all 'Lights' objects as input LPD
    idf$set(Lights := list(watts_per_zone_floor_area = lpd))

    # return the modified model
    idf
}

# create a measure for reducing plug loads during off-work time
set_nightplug <- function (idf, frac = NA) {
    # keep the original if applicable
    if (is.na(frac)) return(idf)

    # extract the plug load schedule into a tidy table
    sch <- idf$to_table("bldg_equip_sch")

    # modify certain schedule value specified using field names
    sch <- sch %>%
        mutate(value = case_when(
            field %in% paste("Field", c(4,14,16,18)) ~ sprintf("%.2f", as.numeric(value) * frac),
            TRUE ~ value
        ))

    # update schedule object using the tidy table
    idf$update(sch)

    # return the modified model
    idf
}

# combine two measures into one
ecm <- function (idf, lpd, nightplug_frac) {
    idf %>% set_lpd(lpd) %>% set_nightplug(nightplug_frac)
}

####################
#  Apply Measures  #
####################

# apply measures and create parametric models
param$apply_measure(ecm,
                  lpd = c(   NA,  7.0,   5.0,        NA,        NA,           5.0),
       nightplug_frac = c(   NA,   NA,    NA,       0.6,       0.2,           0.2),
               # name of each case
               .names = c("Ori", "T5", "LED", "0.6Frac", "0.2Frac", "LED+0.2Frac")
)

# run parametric simulations in parallel
param$run()

#########################
# Data-driven analytics #
#########################

# read building energy consumption from Standard Reports
param_end_use <- param$tabular_data(table_name = "End Uses", wide = TRUE)[[1L]]

# calculate EUI breakdown
param_eui <- param_end_use %>%
    select(case, category = row_name, electricity = `Electricity [kWh]`) %>%
    filter(electricity > 0.0) %>%
    arrange(-electricity) %>%
    mutate(eui = round(electricity / area$'Area [m2]'[1], digits = 2)) %>%
    select(case, category, eui) %>%
    # exclude categories that did not change
    filter(category != "Pumps", category != "Exterior Lighting")

# extract the seed model, i.e. "Ori" case as the baseline
ori_eui <- param_eui %>% filter(case == "Ori") %>% select(-case)

# calculate energy savings based on the baseline EUI
param_savings <- param_eui %>%
    right_join(ori_eui, by = "category", suffix = c("", "_ori")) %>%
    mutate(savings = (eui_ori - eui) / eui_ori * 100) %>%
    filter(case != "Ori")

# plot a bar chart to show the energy savings
p_param_savings <- param_savings %>%
    mutate(case = factor(case, names(param$models()))) %>%
    ggplot(aes(case, savings, fill = category)) +
    geom_bar(position = "dodge", stat = "identity", width = 0.6, color = "black",
        show.legend = FALSE) +
    facet_wrap(vars(category), nrow = 2) +
    labs(x = NULL, y = "Energy savings / %") +
    coord_flip()
```

```{r param-post-process, echo = FALSE, eval = run_sim}
p_param_savings_post <- p_param_savings +
    scale_fill_manual(values = pal_cb[c(7, 6, 3, 1, 8, 2)])
ggsave(here("figures/savings.png"), p_param_savings_post, width = 6, height = 3, dpi = 600)
```

# Code for multi-optimization using Genetic Algorithm

```{r moo, echo = TRUE, eval = run_sim, code.cap = "Multi-objective optimization using Genetic Algorithm"}
# load package
library(epluspar)

# create a GA optimization job
ga <- gaoptim_job(idf, path_weather)

############################
# Optimization objectives  #
############################

# define an objective function to get carbon emissions
carbon_emissions <- function (idf) {
    as.double(idf$last_job()$tabular_data(
        report_name = "emissions data summary",
        row_name = "Annual Sum or average",
        column_name = "carbon equivalent:facility"
    )$value)
}

# define an objective function to get discomfort hours
discomfort_hours <- function (idf) {
    as.double(idf$last_job()$tabular_data(
        table_name = "comfort and setpoint not met summary",
        row_name = "time not comfortable based on simple ASHRAE 55-2004",
        column_name = "facility"
    )$value)
}

# set optimization objectives
ga$objective(carbon_emissions, discomfort_hours, .dir = "min")

###########################
# Optimization variables  #
###########################

# define a measure to change heating setpoint
set_heating_setpoint <- function (idf, sp) {
    sp <- as.character(sp)
    idf$set(htgsetp_sch = list(field_6 = sp, field_16 = sp, field_21 = sp))
    idf
}

# define a measure to change cooling setpoint
set_cooling_setpoint <- function (idf, sp) {
    sp <- as.character(sp)
    idf$set(clgsetp_sch = list(field_6 = sp, field_13 = sp))
    idf
}

# define a measure to change the window-to-wall ratio
set_wwr <- function (idf, wwr) {
    # extract data of all windows
    win <- idf$to_table(class = "FenestrationSurface:Detailed", wide = TRUE, string_value = FALSE)

    # extract data of all parent walls
    wall <- idf$to_table(win[["Building Surface Name"]], wide = TRUE,
        string_value = FALSE, group_ext = "index"
    )

    # calculate new X and Y coordinates for windows
    cols <- sprintf("Vertex %s-coordinate", c("X", "Y", "Z"))
    ratio <- c(0.999, 0.999, wwr)
    cal_coords <- function (coords, ratio) {
        list(round((coords[[1]] - mean(coords[[1L]])) * ratio + mean(coords[[1L]]), 3))
    }
    wall[, .SDcols = cols, by = "id", c(cols) := mapply(
        cal_coords, coords = .SD, ratio = ratio, SIMPLIFY = FALSE
    )]

    # update coordinates of windows
    coords <- wall[, lapply(.SD, unlist), .SDcols = cols, by = "id"]
    coords <- lapply(coords[, -"id"], function (x) as.data.frame(t(matrix(x, nrow = 4))))
    for (axis in c("X", "Y", "Z")) {
        cols <- sprintf("Vertex %i %s-coordinate", 1:4, axis)
        win[, c(cols) := coords[[sprintf("Vertex %s-coordinate", axis)]]]
    }

    idf$update(dt_to_load(win))

    idf
}

# define a measure to change the insulation thickness of the exterior wall
set_insulation <- function (idf, thickness) {
    idf$set(`Steel Frame NonRes Wall Insulation` = list(thickness = thickness))
    idf
}

# combine all measures into one
design_options <- function (idf, htg_sp, clg_sp, wwr, insulation_thickness) {
    idf <- set_heating_setpoint(idf, htg_sp)
    idf <- set_cooling_setpoint(idf, clg_sp)
    idf <- set_wwr(idf, wwr)
    idf <- set_insulation(idf, insulation_thickness)
    idf
}

# specify design space of parameters
ga$apply_measure(design_options,
    htg_sp = choice_space(seq(18, 22, 0.5)),
    clg_sp = choice_space(seq(23, 27, 0.5)),
    wwr = float_space(0.2, 0.8),
    insulation_thickness = float_space(0.02, 0.5)
)

# validate to make sure all measures and objective functions work properly
ga$validate(ddy_only = FALSE)

#################
# GA operators  #
#################

# specify how to mix solutions
ga$recombinator()
# specify how to change parts of one solution randomly
ga$mutator()
# specify how to select best solutions
ga$selector()
# specify the conditions when to terminate the computation
ga$terminator(max_gen = 100L)

# run optimization
ga$run(mu = 20)

# get all population
population <- ga$population()

# get Pareto set
pareto <- ga$pareto_set()

# plot Pareto front
p_pareto <- ggplot() +
    geom_point(aes(carbon_emissions, discomfort_hours), population, color = "darkgoldenrod", alpha = 0.5) +
    geom_line(aes(carbon_emissions, discomfort_hours), pareto, color = "darkblue", linetype = 2) +
    geom_point(aes(carbon_emissions, discomfort_hours), pareto, color = "darkblue", size = 2) +
    scale_x_continuous("Carbon emissions / ton", labels = scales::number_format(scale = 0.001)) +
    scale_y_continuous("Discomfort time based on\nsimple ASHRAE 55-2004 / Hours",
        labels = scales::number_format(big.mark = ",")
    )
```

```{r moo-post-process, echo = FALSE, eval = run_sim}
pareto <- data.table::fwrite(pareto, here::here("data/poreto.csv"))
population <- data.table::fwrite(pareto, here::here("data/population.csv"))

ggsave(here::here("figures/pareto.png"), p_pareto, width = 6, height = 5, dpi = 600)

rng <- tribble(
    ~name                  , ~min        , ~max        ,
    "htg_sp"               , "18 °C"     , "22 °C"     ,
    "clg_sp"               , "23 °C"     , "27 °C"     ,
    "wwr"                  , "20 %"      , "80 %"      ,
    "insulation_thickness" , "0.02 m"    , "0.50 m"    ,
    "carbon_emissions"     , "215.5 ton" , "246.6 ton" ,
    "discomfort_hours"     , "648 hours" , "3507 hour"
)

normalize <- function (x, min = NULL, max = NULL) {
    if (is.null(min)) min <- min(x)
    if (is.null(max)) max <- max(x)
    (x - min) / (max - min)
}

p_parallel <- pareto %>%
    mutate(
        htg_sp = normalize(htg_sp, 18, 22),
        clg_sp = normalize(clg_sp, 23, 27),
        wwr = normalize(wwr, 0.2, 0.8),
        insulation_thickness = normalize(insulation_thickness, 0.02, 0.5),
        carbon_emissions = normalize(carbon_emissions),
        discomfort_hours = normalize(discomfort_hours)
    ) %>%
    pivot_longer(-index) %>%
    mutate(index = as_factor(index), name = as_factor(name)) %>%
    ggplot(aes(name, value, group = index, color = index)) +
    geom_segment(aes(x = name, xend = name, y = 0.0, yend = 1.0), color = "grey80") +
    geom_hline(aes(yintercept = 1.0), color = "grey80") +
    geom_hline(aes(yintercept = 0.0), color = "grey80") +
    geom_line(show.legend = FALSE, alpha = 0.5, size = 1.2) +
    geom_point(show.legend = FALSE, alpha = 0.5, size = 2.5) +
    geom_text(aes(name, 1, label = max), rng, vjust =-1.0, inherit.aes = FALSE, color = "grey50", fontface = "bold") +
    geom_text(aes(name, 0, label = min), rng, vjust = 2.0, inherit.aes = FALSE, color = "grey50", fontface = "bold") +
    scale_x_discrete(NULL, position = "top", expand = expand_scale(0.05, 0.05),
        labels = c("Heating\nSetpoint", "Cooling\nSetpoint", "WWR", "Insulation\nThickness", "Carbon\n Emissions", "Discomfort\nHours")
    ) +
    scale_y_continuous(expand = expand_scale(mult = c(0.05, 0.08))) +
    scale_color_viridis_d() +
    theme(
        axis.text.x = element_text(size = 12, vjust = -2.0, face = "bold", color = "grey30"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = NA)
    )

ggsave(here("figures/parallel.png"), p_parallel, width = 10, height = 6, dpi = 600)
```

# Code for model calibration using Bayesian theory

```{r bc, echo = TRUE, eval = run_sim, code.cap = "Bayesian calibration"}
################################################################################
# NOTE: for demonstration, we use the seed model to generate some synthetic data
# clone the original model
tmp <- idf$clone()
# remove all existing run periods
tmp$RunPeriod <- NULL
# add a new run period from Jul 1st to Jul 3rd
tmp$add(RunPeriod = list("test", 7, 1, NULL, 7, 3))
# add variables of interest to output
tmp$add(Output_Variable = list("VAV_1_Fan", "Fan Electric Power", "Hourly"))
# get rid of design day variable output data
tmp$SimulationControl$set(run_simulation_for_sizing_periods = "No")
# save the model to a temporary file
tmp$save(tempfile(fileext = ".idf"))
# run simulation
job <- tmp$run(path_weather)
# extract fan electric power in 6-hourly frequency
fan_power <- job$report_data(name = bc$output()$variable_name, all = TRUE) %>%
    epluspar:::report_dt_aggregate("6 hour") %>%
    eplusr:::report_dt_to_wide()
# insert Gaussian noise
fan_power <- fan_power %>%
    select(-`Date/Time`) %>%
    rename(power = everything()) %>%
    mutate(power = power + rnorm(length(power), sd = 0.05 * sd(power))) %>%
    mutate(power = case_when(power < 0.0 ~ 0.0, TRUE ~ power))
################################################################################

# load library
library(epluspar)

# create a `BayesCalibJob` object:
bc <- bayes_job(idf, path_weather)

# specify parameters that can be measured
bc$input("VAV_1 Supply Equipment Outlet Node", "System Node Mass Flow Rate", "Hourly")

# specify the parameter to predict
bc$output("VAV_1_Fan", "Fan Electric Power", "Hourly")

# specify parameters to calibrate
bc$param(
    VAV_1_Fan = list(fan_total_efficiency = c(0.4, 0.8)),
    .num_sim = 30, .names = "FanEfficiency"
)

# get sample parameter values generated using Latin Hypercube Sampling (LHS)
bc$samples()

# run simulations from Jul 1st to Jul 3rd
bc$eplus_run(run_period = list("example", 7, 1, NULL, 7, 3))

# gather simulated data in 6-hour time frequency
bc$data_sim("6 hour")

# set field data
bc$data_field(fan_power)

# get input data for Stan
bc$data_bc()

# run Bayesian calibration using Stan
res <- bc$stan_run(iter = 300, chains = 3)

# extract posterior distributions of calibration parameter
dist <- bc$post_dist()

# extract prediction values
pred <- bc$prediction()

# evaluate the uncertainties including NMBE and CV(RMSE)
uncert <- bc$evaluate()

# draw a density plot for the posterior distributions of calibration parameters
p_dist <- dist %>%
    pivot_longer(-sample) %>%
    ggplot() +
    geom_density(aes(value, fill = name), alpha = 0.5) +
    geom_vline(aes(xintercept = mean(value)), linetype = 2, size = 1)

# draw a boxplot to show the distributions of uncertainty indices
p_uncert <- uncert %>%
    pivot_longer(-sample) %>%
    ggplot() +
    geom_boxplot(aes(name, value, fill = name), alpha = 0.5)
```

```{r bc-post-process, echo = FALSE, eval = run_sim}
p_dist_post <- p_dist +
    scale_x_continuous("Fan total efficiency") +
    geom_text(aes(x = mean(value) * 1.001, y = 80, label = paste0("Mean:", scales::number(mean(value), 0.01))), hjust = -0.2) +
    scale_fill_manual(values = pal_cb[3]) +
    theme(legend.position = "none")
ggsave(here("figures/dist_post.png"), p_dist_post, width = 6, height = 3.5, dpi = 600)

p_uncert_post <- p_uncert +
    scale_x_discrete(NULL, labels = c("CV(RMSE)", "NMBE")) +
    scale_y_continuous(NULL, labels = scales::percent_format(0.01)) +
    scale_fill_manual(values = pal_cb[c(4, 5)]) +
    theme(
        legend.position = "none",
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)
    )
ggsave(here("figures/uncertainties.png"), p_uncert_post, width = 6, height = 3, dpi = 600)
```

`\end{appendices}`{=latex}

\clearpage

# References {#references .unnumbered}
