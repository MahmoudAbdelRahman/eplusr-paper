---
title: "eplusr: A framework for integrating building energy simulation and data-driven analytics"
author:
  - name: Hongyuan Jia
    email: hongyuan.jia@bears-berkeley.sg
    affiliation: SinBerBEST
  - name: Adrian Chong
    email: adrian.chong@nus.edu.sg
    affiliation: NUS
    footnote: 1
address:
  - code: SinBerBEST
    address: |
      SinBerBEST Program, Berkeley Education Alliance for Research in Singapore,
      Singapore, 138602, Singapore
  - code: NUS
    address: |
      Department of Building, School of Design and Environment, National
      University of Singapore, 4 Architecture Drive, Singapore, 117566,
      Singapore
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  Building energy simulation (BES) has been widely adopted for the
  investigation of environmental and energy performance for different design
  and retrofit alternatives. In BES, data-driven analytics is vital for
  interpreting and analyzing BES results to reveal trends and provide useful
  insights. However, seamless integration between BES and data-driven analytics
  current does not exist. This paper presents 'eplusr' for conducting
  parametric analysis using EnergyPlus via the R programming language.
  With a data-centric design
  philosophy, the proposed framework focuses on better and more seamless
  integration between BES and data-driven analytics. It provides a Tidy data
  format for BES that matches the semantics of the simulation results which can
  be easily fed to various data analytics workflows in R. The framework also
  provides an infrastructure to bring portable and reusable computation
  environment for building energy modeling using, which aims to facilitate the
  reproducibility research in building energy domain.
journal: "Energy and Buildings"
date: "`r Sys.Date()`"
bibliography: references.bib
csl: elsevier-with-titles.csl
layout: 3p, times
colorlinks: true
link-citations: true
linenumbers: true
output:
  bookdown::pdf_book:
    includes:
      in_header: header.tex
    base_format: rticles::elsevier_article
    keep_tex: yes
---

```{r setup, include = FALSE}
library(eplusr)
library(kableExtra)
library(here)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  eval = FALSE,
  comment = "#>",
  out.width = "\\columnwidth",
  fig.path = "../figures/",
  fig.pos = "!htb"
)

# code chunk cross-ref
Chunk <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function (x, options) {
    x <- Chunk(x, options)
    if (is.null(options$label)) return(x)
    if (!options$echo) return(x)
    if (!is.null(options$fig.cap)) return(x)
    # use chunk label as cross-ref label
    x <- paste0(
        # use Pandoc raw-attribute to preserve raw LaTeX content
        # https://pandoc.org/MANUAL.html#extension-raw_attribute
        "`\\begin{Shaded}`{=latex}\n",
        x, "\n",
        "\\captionof{code}{", options$code.cap, "\\label{code:", options$label, "}}\n",
        "`\\end{Shaded}`{=latex}\n"
    )
    x
})

# set to TRUE to run simulation code chunks
run_sim <- FALSE
```

# Highlights {.unnumbered}

1. Developed an R package for seamless EnergyPlus data analytics applications
2. Structures inputs/outputs format for seamless integration with data analytics workflow
3. Facilitates reproducible simulations through Docker with R project management
4. Enables flexible and extensible parametric simulations

# Introduction

Building energy simulation (BES) is increasingly being used throughout the
building's life-cycle for the analysis and prediction of building energy
consumption, measurement and verification, carbon evaluation and cost analysis
of energy conservation measures (ECMs) [@Chong2017; @Kneifel2010].
It has played a growing role in the design and operation of low energy,
high-performance buildings, and development of policies that drive the
achievement of reducing energy use and greenhouse-gas emissions in the
buildings sector [@Hong2018].

BES offers an alternative approach that encourages customized, integrated
design solutions, and the development of BES tools has been pronounced over the
decades [@Hong2000; @Hong2018].
The core tools in the BES field are the whole-building energy simulation
programs that provide users with key building performance indicators such as
energy use and demand, temperature, humidity, and costs [@Crawley2008a].
However, BES, with an iterative nature inside, can produce a large amount
of data. The volumes of BES data have overwhelmed traditional data
analysis methods such as spreadsheets and ad-hoc queries with a large number of
factors to be considered [@Kim2011].
Solutions in most existing frameworks have limited post-processing capacities
on BES results, and are not flexible enough to enable a clear understanding and
control on how the BES data is being transformed [@Miller2013; @Attia2013a].
According to a survey of 448 building energy management professionals in the
U.S., there is a need of improving the efficacy and integration of data-driven
analytics and BES in the building energy management domain, and efforts should
be made to develop integrated tools that are capable of leveraging both methods
[@Srivastava2019].

As BES becomes more and more integral to many aspects of architecture design,
and decision-making processes, computational reproducibility has become an
issue of increasing importance to researchers, designers and practitioners.
Lack of credibility in BES results due to a lack of quality and reproducibility
is widely considered a problem by the energy model community [@Fleming2012].
Despite the culture of reluctance and a lack of requirements or incentives to
publish the code used in generating the results [@Boettiger2015], the
reproducible issue in BES is mainly caused by the absence of (1) an integrated
workflow between BES and data-driven analytics and (2) a portable and reusable
computation environment encapsulating essential software and applications to
perform it.

To address those issues, in this paper, we introduce a new framework for
integrating BES and data-driven analytics.
The framework is different from existing ones because of its data-centric
design philosophy.
The objectives are (1) to provide better and more seamless integration between
BES engine EnergyPlus and R-programming data-driven analytics environment
through a parametric simulation prototype together with a tidy data
[@Wickham2014] interface and (2) to build infrastructures for portable and
reusable BES computation environment to facilitate reproducibility research
in building energy domain.
This paper first introduces the concepts and behind the framework and its
modules, along with its implementation. Next, the applications of the framework
using a medium office building model are presented, covering various topics
including data exploration, parametric simulation, optimization and
calibration.

# State of the art

## Parametric simulation frameworks

Parametric analysis in BES has been a powerful approach to enable investigation
of environmental and energy performance for different design and retrofit
alternatives.
Choosing the appropriate combination of design options is a complex task that
requires the management of a large amount of information on the properties of
design options and the simulation of their performance [@Purup2020].
However, parametric analysis involves tedious file management tasks, repeated
entry of model parameters, the application of design transformations and the
execution of large-scale analyses [@Macumber2012], which can be time-consuming
and error-prone.
Parametric simulation task automation has been proven to be an extremely useful
way to reduce human intervention and improve the efficiency of large parametric
analysis [@Roth2018].
As shown in Table \@ref(tab:pat-sum), this led to the development of various
free parametric simulation frameworks based on EnergyPlus [@Crawley2001], one
of the widely used whole building energy simulation tools.

<!--
knitr::kable does not supprt references in cells.
Have to use text cross-reference feature brought in bookdown.
It's ugly but works

Ref: https://github.com/haozhu233/kableExtra/issues/214
-->

(ref:pat) [@Guglielmetti2011]
(ref:ladybug) [@Roudsari2013]
(ref:jeplus) [@Yi2020]
(ref:modelkit) [@BigLadderSoftware2020]
(ref:mle) [@Bernal2012]
(ref:epxl) [@Schild2020]
(ref:genopt) [@Wetter2001]
(ref:eppy) [@Philip2020]

\setlength{\tabcolsep}{0.1pt}
\renewcommand{\arraystretch}{1.3}
```{r pat-sum, eval = TRUE, include = TRUE, echo = FALSE}
compare <- data.table::fread(here::here("data/compare.csv"))

# use tick symbol
compare[, c(names(compare)) := lapply(.SD, function (val) {val[val == "X"] <- "\\cmark";val})]

# columns that should be further described in footnotes
cols <- c("Semantic API", "Supports optimization", "Data extraction", "Weather data handling", "Post-processing capabilities")
data.table::setnames(compare, cols, paste0(cols, kableExtra::footnote_marker_number(seq_along(cols), "latex")))

compare %>%
    .[Name == "Ladybug & Honeybee",  Name := "Ladybug \\& Honeybee"] %>%
    knitr::kable(booktabs = TRUE, format = "latex", linesep = "", escape = FALSE,
        align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
        caption = "Comparison of parametric simulation frameworks based on EnergyPlus") %>%
    kableExtra::kable_styling(font_size = 7, latex_options = "hold_position") %>%
    # have to manually specify the column width since 'full_width' cannot be used here
    # ref: https://github.com/haozhu233/kableExtra/issues/39
    # name
    kableExtra::column_spec(1, "2.9cm") %>%
    # cross-platform
    kableExtra::column_spec(2, "1.2cm") %>%
    # GUI
    kableExtra::column_spec(3, "0.45cm") %>%
    # Open-source
    kableExtra::column_spec(4, "1.1cm") %>%
    # Semantic API
    kableExtra::column_spec(5, "1.0cm") %>%
    # API language
    kableExtra::column_spec(6, "1.2cm") %>%
    # Support optimization
    kableExtra::column_spec(7, "1.5cm") %>%
    # Support calibration
    kableExtra::column_spec(8, "1.5cm") %>%
    # Data extraction
    kableExtra::column_spec(9, "1.4cm") %>%
    # SQL-based structural output
    kableExtra::column_spec(10, "1.5cm") %>%
    # Weather data handling
    kableExtra::column_spec(11, "1.3cm") %>%
    # Post-process capabilities
    kableExtra::column_spec(12, "1.6cm") %>%
    # BIM Interoperability
    kableExtra::column_spec(13, "1.6cm") %>%
    kableExtra::add_footnote(threeparttable = TRUE, notation = "number", c(
        # Semantic API
        "Further abstraction classes to directly perform geometry transformations, HVAC system manipulation, etc.",
        # Support optimization
        "Only built-in features are considered. So as for calibration support. Some frameworks can be further coupled with other software or libraries to perform optimizations",
        # Data extraction
        "The capabilities of extracting further customized summary data, instead of solely based on EnergyPlus built-in functionalities",
        # Weather data handling
        "The capabilities of extracting and modifying data from weather files",
        # Post-processing capabilities
        "The capabilities of performing further data analyses on the extracted simulation results. 'H', 'M' and 'L' indicates 'High', 'Medium' and 'Low', respectively"
    )) %>%
    # change footnote font size manually
    gsub("\\small", "\\scriptsize", x = ., fixed = TRUE)
```

Some frameworks in Table \@ref(tab:pat-sum) consist of graphical user
interfaces (GUIs) built on EnergyPlus while some use general-purpose scripting
languages accompanied by a suite of programming features and libraries
[@Roth2018].
OpenStudio [@Guglielmetti2011] is a free open-source software toolkit designed
for energy modeling and can be used to efficiently create or modify models,
manage individual or multiple simulations, and visualize results.
OpenStudio has its own format (`.OSM`) and schema for EnergyPlus model
representation which will eventually be translated into EnergyPlus models.
Parametric Analysis Tools (PAT) is a GUI application that is part of the OpenStudio
toolkit. It aims to enable customizable and shareable parametric descriptions
of ECMs [@Parker2014]. It leverages OpenStudio Measures which are reusable
scripts written in Ruby programming language to manipulate OpenStudio models
and can be used to compare manually specified combinations of measures,
optimize designs, calibrate models and perform parametric sensitivity analysis.
Ladybug and Honeybee are plug-ins developed for Rhino Grasshopper
[@Roudsari2013; @Tabadkani2019]. Ladybug is used to import and analyze Energy
standard weather data (EPW) while Honeybee is used to create, run, and
visualize OpenStudio and EnergyPlus simulation results.
Both OpenStudio and Honeybee have built further abstractions that are capable
of performing building geometry transformation and restructuring HVAC (Heating,
Ventilation and Air-Conditioning) systems, and provide supports for BIM
(Building Information Modeling) interoperability.
Since the primary Input Data Files (IDFs) of EnergyPlus are all ASCII
text-based, several applications directly update the building energy models by
processing and manipulating the text files, without taking into account the
complex hierarchical structure in the model components.
jEplus [@Yi2020] is a software written in Java programming language to perform
complex parametric analysis on multiple design parameters. It allows users to
describe the parameters and their values using customized symbols via a GUI and
automatically create parametric models using text-substitution [@Zhang2010a].
Modelkit [@BigLadderSoftware2020] automates the generation and management of
EnergyPlus models via its templates and scripting tools written in Ruby.
These frameworks are designed for simplicity and flexibility and are mainly
focusing on generating parametric models based on an existing seed model,
instead of creating a new one from scratch.
For further customized automation tasks, several frameworks are interfacing
EnergyPlus with scripting programming languages.
MLE+ integrates EnergyPlus and scientific computation and design capacities of
Matlab for controller design and can be used to implement and simulate advanced
control algorithms of building systems [@Bernal2012; @Zhao2013b].
EpXL [@Schild2020] is an EnergyPlus Microsoft Excel user-interface written in
Visual Basic for Applications (VBA) that enables import and export of IDF data
files, parametric analysis and optimization. It is capable of displaying a
compact tabular overview of input data and automatically importing simulation
output into Excel, with a link for viewing the 3D model.
eppy [@Philip2020] is a library for interfacing EnergyPlus with Python
programming language. It parses EnergyPlus models into a Python object and
provides low-level programmatic access to EnergyPlus inputs.
<!-- transition -->
GenOpt is a generic optimization program that can be used with EnergyPlus
with respect to multiple parameters [@Wetter2001]. It generates new input
templates files by replacing the keywords with corresponding numerical values
calculated using a mathematical optimization library.
The frameworks mentioned above may have overlappings in features but they are
tailored for different purposes and use cases, with the primary focus on ease
the time-consuming and error-prone process of creating and managing parametric
simulations.

## Data-driven analytics of BES data

The use of data-driven analytics techniques to increase energy efficiency is
attracting a great deal of attention and interest [@Molina-Solana2017].
Currently, there is a growing body of scientific literature on the application
of advanced mathematical algorithms for building design using BES [@Kiss2020].
Generally, data-driven analytics encompasses the whole data analysis process
beginning with data extraction and cleaning, and extends to data analysis,
description and summarization [@Molina-Solana2017; @BurakGunay2019].
Most of the time, data analytics of BES results is of commensurate importance
with the model itself.
However, the output of common BES tools is not always friendly in format for
applying these methods which makes data pre-processing an essential but
time-consuming and laborious process for any data-driven analytics for BES
data.
This highlights the potential areas for improvements in data extraction and
result presentation in a clear and intuitive manner for data analytics.
Even some BES tools provide summary reports with various details, it is still
quite common in BES to perform post-processing and apply customized and more
advanced algorithms to the simulation results.
Unfortunately, most existing frameworks listed in Table \@ref(tab:pat-sum) have
limited capacities with this regard.
Open-source programming environments such as R [@RCoreTeam2019] and Python
[@Oliphant2007] are promising on providing solutions for large-scale data
analytics and have become widely-used research tools that provide access to
many well-documented packages for various data mining, machine learning and
data visualization applications [@Lowndes2017; @Molina-Solana2017].
Even though the recent survey [@Srivastava2019] has highlighted the urgent need
for an integrated solution, fewer efforts have been made in terms of providing
a seamless integrated approach to bridge the gap between BES and data-driven
analytics.

## Reproducible research of BES

BES is a complex domain that involves multiple scientific processes and is
often a hard-to-reproducible process.
Reproducibility is defined as the ability to recompute data analytic results,
given an observed data set and knowledge of the data analysis pipeline
[@Peng2015]. Reproducible research has received an increasing level of
attention throughout the scientific community and the public at large
[@Boettiger2015]. According to a Nature's survey of 1576 researchers in 2016,
more than 70% of them failed to reproduce another scientist's experiments, and
more than 50% failed to reproduce their own experiments. Moreover, more than
half of them agreed that there was a significant crisis of reproducibility
[@Baker2016a].
Currently, improving computational reproducibility has become an important
step to increase the credibility in BES results [@Fleming2012].
The reasons for the BES reproducible issue are two-fold:
(1) missing seamless integration between simulation and data analysis workflows and
(2) absence of a portable and reusable computation environment.
Most users prefer to use GUI applications which makes it intuitive and easy to
execute certain particular tasks.
However, GUI tools have constraints on flexibility as the users have to specify
exactly what and how features of the design can be manipulated and often are
not be able to provide a good workflow for repeating that task across a wider
range of situations on different systems.
In this case, manual steps have to be performed using other tools, such as a
spreadsheet or command-line tools, which introduces additional transcription
burden and results in a non-reproducible process [@Macumber2012].
Sometimes, custom solutions have to be created from scratch to automate part
portions of the workflows, which may lead to new inefficiencies and potential
errors. Currently, no widely adopted solution is able to
integrate all processes into one single platform. Moreover,
BES often involves the use of multiple applications, software and platforms.
To perform crucial scientific processes such as replicating the results,
extending the approach or testing the conclusions in other contexts, the
indispensable step is to install the software used by the original researchers,
which sometimes can become immensely time-consuming if not impossible.
It is easy to underestimate the significant barriers raised by a lack of
familiar, intuitive, and widely adopted tools for addressing the challenges of
computational reproducibility [@Boettiger2015].

# Methodology

To achieve seamless integration between BES and data-driven
analytics, a framework shown in Fig. \@ref(fig:architecture) has been
developed, which contains three main components:

1. Tidy data [@Wickham2014] interface for structuring BES inputs and outputs for seamless
   integration with data analytics workflow
2. Parametric prototype for conducting flexible and extensible parametric simulations
3. Computation environment based on Docker containerization [@Merkel2014] for
   reproducible BES to facilitate reproducibility research in BES domain

The first two components have been packaged into a free open-source library
"eplusr"^[GitHub Repository: https://github.com/hongyuanjia/eplusr] written in
R programming language, while the last has been encapsulated using Docker
containerization^[Docker Hub Link:
https://hub.docker.com/r/hongyuanjia/eplusr].
A further detailed description of the tidy data interface (Section
\@ref(sec:eplusr-tidy)), the parametric simulation prototype (Section
\@ref(sec:eplusr-parametric)), and the concepts and components
of the Docker containerization for BES (Section \@ref(sec:docker)) are introduced.


```{r architecture, echo = FALSE, eval = TRUE, fig.cap = "An architecture overview of the proposed framework which includes three main components: (1) Tidy data interface, (2) Parametric Prototype and (3) Computation environment for reproducible BES using Docker containerization", out.width = "40%"}
knitr::include_graphics(here("figures/overview.png"))
```

## Tidy data interface {#sec:eplusr-tidy}

The concept of tidy data format was first proposed by Wickham [@Wickham2014],
which is a standard way of mapping the meaning of a dataset to its structure.
In tidy data, each variable forms a column, each observation forms a row and
each type of observational unit forms a table.
This structure makes it easy for an analyst or a computer to extract needed
variables and is particularly well suited for vectorized programming languages
like R, because the layout ensures that values of different variables from the
same observation are always paired [@Wickham2014; @Wickham2017] and is well
fitted for directly handling by the *tidyverse* ecosystem, which is a language for
solving data-driven analytics challenges using R [@Wickham2019].

Table (a) in Fig. \@ref(fig:tidy-format) shows an example of the standard
format from EnergyPlus CSV table output, while Table (b) gives the tidy
representation of the same underlying data using the Tidy data interface.
Through the structure of Table (a) provides efficient storage for completely
crossed designs, it violates with the Tidy principles, as variables form both
the rows and columns and column headers are values, not variable names.
Several values are concatenated in column headers, including variable `Key
Value`, `Variable Name`, `Units` and `Reporting Frequency`.
Additional data cleaning efforts are needed to work with this structure,
especially considering the missing values (`NA` in row 2 and 4 in Table (a))
introduced by the aggregation of various reporting frequencies, which may
add new inefficiencies and potential errors.
In Table (b), values in column headers have been extracted and converted into
separate columns and a new variable called `Value` is used to store the
concatenated data values from the previously separate columns.
Moreover, instead of presenting date and time as strings in Table (a), the Tidy
data interface splits its components into four new variables, including
`Month`, `Day`, `Hour` and `Minute`.
Taken together, Table (b) forms a nine-variable tidy table and each variable
matches the semantics of simulation output.
Considering the times of data analysis operations to be performed on the values
in a variable, the advantage of structuring values in a simple and standard way
stands out. It can facilitate initial exploration and analysis of data and to
simplify the development of data analysis tools that work well together
[@Wickham2014].

```{r tidy-format, echo = FALSE, eval = TRUE, fig.cap = "An example of tidy BES output data representation using Tidy data interface where Table (a) is the standard output format of EnergyPlus CSV table and Table (b) is the tidy representation of the same underlying data using the Tidy data interface", out.width = "60%"}
knitr::include_graphics(here("figures/tidy-format.png"))
```

The tidy data interface is implemented through three modules shown in Fig.
\@ref(fig:architecture), including
(1) I/O Processing & Parser Module for tidy data representation of BES inputs,
(2) Model API Module for tidy data model modification APIs, and
(3) Tidy Data Extractor Module for querying and structuring BES outputs in tidy format.
Each will be described in the following sections.

### Tidy data representation of BES inputs

The I/O processing & parser module shown in Fig. \@ref(fig:architecture) aims
to read, parse and represent EnergyPlus models and weathers in relational tidy
format tables, providing the flexibility to extract and modify BES models and
weathers using structured data toolkits available in R.

EnergyPlus Input Data File (IDF) is based on the data schema that is defined in
Input Data Dictionary (IDD).
In the proposed framework, data of an IDF and the corresponding IDD are stored
as Relational Databases (RD) and encapsulated into an `Idf` object and `Idd`
object, respectively.
RD was first proposed by Codd [@Codd1990] and has become the dominant database
model for a number of Relational Database Management Systems (RDMS).
It organizes data in a set of rectangular tables with rows and columns.
Each table has a primary key which is a unique identifier constructed
from one or more columns and a table is linked to another by including the
other table's primary key which is called a foreign key for that table.
Fig. \@ref(fig:data-structure) shows the relationships among tables in an `Idf`
and `Idd` object with the primary keys presented in light blue color.
The RD data structure follows the idea of database normalization where each
fact is expressed in only one place, similar to the Tidy principle.
This feature helps keep the data integrity and reduce any data redundancy and
enables fast table joining among entities and variables.
To modify an IDF is equal to change the data in `Idf` tables accordingly, in
the context of specific `Idd` data.

```{r data-structure, echo = FALSE, eval = TRUE, fig.cap = "Data structure of an Idf and Idd object", out.width = "70%"}
knitr::include_graphics(here("figures/data_structure.png"))
```

### Tidy data input API for BES model modifications

The Model API module enables to perform queries and modifications on EnergyPlus
models programmatically.
Flexibly-scoped interfaces have been developed to modify field values in a
model, including single-object level, grouped-object level, and whole-class
level, enabling to alter a number of objects at the same time.
Both `Idf` and `IdfObject` class provide a `to_table()` method to extract
certain or all parts of a model into one tidy `data.table` object, which is an
extension of R's table representation but extremely optimized for fast
computation [@Dowle2019].
The `load()` and `update()` methods in `Idf` class can take that tidy data as
input, and respectively create and modify models accordingly.
Thus, the Model API module provides a novel model-editing workflow with
tidy-data centered, composed by table extraction, transformation, and insertion.

### Tidy data representation of BES inputs

The Tidy Data Extractor module is designed to extract and represent EnergyPlus
simulation outputs in relational tidy format tables.
It introduces a `EplusSql` class that utilizes EnergyPlus SQLite output format
to extract simulation results.
SQLite is a mature and widely-employed RDMS [@Owens2006].
The main benefits of using the EnergyPlus SQLite output format are that it
contains all of the data in standard reports, variable output, meter outputs
and a number of standard input and output summaries, and thus makes it possible
to build tidy-data interfaces to retrieve various kinds of simulation results
using a single file.

Fig. \@ref(fig:tidy-extractor) shows an implementation overview of the data
extractor for EnergyPlus variable and meter outputs.
The primary keys are highlighted with light blue color in and fields that can
be specified through the interface are highlighted with light purple color.
Interfaces have been developed to retrieve outputs of given variables and
meters with certain key identifiers, at a specified time, simulation run period,
reporting frequency and unit.
It is achieved by sending SQL (Structured Query Language) queries, a
domain-specific language for RDMS, to the simulation output SQLite database.
The results are outcomes of joining operations on four tables, including
`Time`, `EnvironmentPeriods`, `ReportDataDictionary` and `ReportData`.
However, the time components in the SQL outputs fail to assemble complete
time-series data, due to missing a year specification^[A `Year` field was added in the recent version of EnergyPlus. But old versions of EnergyPlus are still widely used.], making it
impossible to directly apply time-series-based algorithms. To solve this issue,
a year derivation algorithm is implemented that calculates a proper year value
for each run period based on the date and time components, and compose a complete
series of `POSIXct` values, which is the standard date-time class in R.

```{r tidy-extractor, echo = FALSE, eval = TRUE, fig.cap = "Overview of the tidy data extractor for variable and meter outputs", out.width = "80%"}
knitr::include_graphics(here("figures/result_extraction_interface.png"))
```

## The parametric prototype {#sec:eplusr-parametric}

The parametric prototype in the framework provides a set of abstractions to
ease the process of parametric model generation, design alternative evaluation
and large parametric simulation management. An overview of the parametric
prototype implementation is shown in Fig. \@ref(fig:parametric).

```{r parametric, echo = FALSE, eval = TRUE, fig.cap = "Workflow of a parametric simulation", out.width = "70%"}
knitr::include_graphics(here("figures/parametric.png"))
```

A parametric simulation is initialized using a seed model and a weather file.
Design alternatives are specified by applying a *measure* function to the seed
model. The concept of *measure* in the prototype is inspired by a similar concept
in OpenStudio [@Guglielmetti2011] but tailored for flexibility and
extensibility.
A measure is simply an R function that takes an `Idf` object and any other
parameters (e.g. $t_1$ to $t_5$ in Fig. \@ref(fig:parametric)) as input, and
returns a set of modified `Idf` objects as output, making it possible to
leverage other modules in the framework and apply statistical methods and
libraries existing in R to generate design options.
After a measure is defined, the method `apply_measure()` takes it and other
parameter values specified to create a set of models, and the `run()` method
will run all parametric simulations in parallel and place each simulation
outputs in a separate folder.
All simulation metadata will keep updating during the whole time and can be
retrieved using the `status()` method for further investigations.

The `ParametricJob` class leverages the Tidy data interface to retrieve parametric simulation
results in tidy format. Despite that, a number of methods are also provided
to read various output files, including simulation errors (`eplusout.err`), report data
dictionary (`eplusout.rdd`) and meter data dictionary (`eplusout.mdd`). For all
resulting tidy tables, an extra column containing the simulation job identifiers
is prepended in each table and can be used as an index or key for further data
transformations, analyses and visualization to compare simulated design options.

The proposed parametric prototype is designed to be simple yet flexible and
extensible. One good example of the extensibility of this framework is the
epluspar^[GitHub Repository: https://github.com/hongyuanjia/epluspar] R package, which provides new classes for conducting
specific parametric analysis on EnergyPlus models, including sensitivity
analysis using the Morris method [@Morris1991] and Bayesian calibration using the
method proposed by @Chong2017. All the new classes introduced are based on the
`ParametricJob` class. The main difference mainly lies in the specific
statistical method used for sampling parameter values when calling
`apply_measure()` method. Few examples of this application have been provided
in Section \@ref(sec:applications).

## Computation environment for reproducible BES {#sec:docker}

The Docker containerization for BES
aims to provide infrastructure to bring portable and reusable computation
environment to facilitate the reproducible BES application. Peng [@Peng2015]
summarized two major components to successful reproducible research: (1)
data, i.e. the availability of raw data from the experiment, and (2) code, i.e.
the availability of the statistical code and documentation to reproduce the
results. In the context of BES, these will be (1) the building energy models
and (2) the code to perform simulations and following data-driven analytics.
However, the complex and rapidly changing nature of computer environments
makes it immensely challenging to reproduce the same workflow and results even
with the original data and code are available. To address this issue, a
reproducible BES computation environment has been developed based on the Docker
containerization technology, enabling to capture the full software stack
including all software dependencies in a portable and reusable image.

Docker [@Merkel2014] is a popular open-source tool for containerization and has
shown its potential to improve computational reproducibility [@Boettiger2015;
@Nust2020]. The Rocker Project was launched in 2014 as a collaboration to
provide high-quality Docker images containing the R environment, and has seen
both considerable uptakes in the R community and substantial development and
evolution [@Boettiger2017]. The proposed reproducible BES computation
environment is built upon the `rocker/verse` images and contains four groups of
toolchains needed for common BES and data-driven analytics workflows using the
eplusr framework:

1. Statistical computing environment, including the latest R environment and
   RStudio Server, a web-based integrated development environment for R
   programming
1. BES engine, including EnergyPlus of specified version and the eplusr R
   package
1. Data analytics toolkits, including a collection of tidyverse [@Wickham2019]
   R packages for data import, tidying, manipulation, visualization and
   programming
1. Literate programming environment, including R Markdown related packages
   for dynamic document generation

The first three have been described in previous sections. Literate programming
is a programming paradigm introduced by Knuth [@Knuth1984] in which the explanation of
a computer program is given, together with snippets of source code. Recently,
there have been significant efforts to develop literate programming
infrastructure to reproducibly perform and communicate data analyses, including
R Markdown [@Grolemund2018] , Jupyter notebook [@Kluyver2016], just to name a
few.

The R Markdown format is underneath powered by the knitr R package [@Xie2015]
and Pandoc [@Krewinkel2017]. Knitr executes the computer code written in
various programming languages embedded, and converts R Markdown to Markdown.
And Pandoc processes the resulting Markdown and render it to various output
formats, including PDF, HTML, Word, etc. The R Markdown format has been a
widely adopted authoring framework for data science. It can be used to both
save and execute code, and generate high-quality reports that can be shared
with an audience. Together with rmarkdown [@Allaire2020] and tinytex [@Xie2019]
packages, the proposed BES computation environment can be easily adapted to any
R-centric workflows and enables researchers in the BES field to build and archive
reproducible analytics.

The source files of Docker configuration were written in several text files
so-called Dockerfiles and are publicly available and hosted via GitHub^[GitHub
Repository: https://github.com/hongyuanjia/eplusr-docker]. Further
evolutions can be taken to make the computation environment tailored to
different audiences and use purposes. The docker approach is
suited for moving between local and cloud platforms when a web-based integrated
development environment is available, such as RStudio Server [@Boettiger2015],
providing the scalability potential for large cloud-based BES computation.

# Applications {#sec:applications}

To show how the eplusr framework can be used, examples are presented in four
topics: (1) data exploration, (2) parametric simulation, (3) multi-objective
optimization using Genetic Algorithm (GA), and (4) calibration using Bayesian
theory. For all examples, the U.S. Department of Energy (DOE) medium office
reference building model in compliance with Standard ASHRAE 90.1 -- 2004
[@Field2010] is used. Fig. \@ref(fig:medium-office) shows a 3D view of the
building geometry. It represents a 3-story, 15-zone office building with a total
floor area of 4982 $\mathrm{m}^2$. Each floor was divided into 1 core zone and
4 perimeter zones. Three multi-zone variable air volume (VAV) with reheat coil
systems, with each served by a direct-expansion (DX) cooling coil and a gas
burner, were installed to provide the conditioned thermal environment for each
floor. The typical meteorological year 3 (TMY3) weather data of Chicago was
used for all the simulations.

```{r medium-office, echo = FALSE, eval = TRUE, fig.cap = "3D view of DOE medium office reference building", out.width = "40%"}
knitr::include_graphics(here::here("figures/medium-office.png"))
```

## Data exploration on the EUI and the heating and cooling demands {#sec:basic}

The energy use intensity (EUI) is one key indicator for building energy
performance and its breakdown can provide potential directions of where ECMs
should be applied to reduce energy usage. When evaluating the feasibility of
free-cooling applications in buildings, the heating and cooling demand profile
plays an important role in the determination of the potential. This example
demonstrates the data exploration process of obtaining these two outcomes from
annual simulation results. It showcases the basic features of the eplusr
framework with the main focus on how the tidy data extractor can provide a seamless
workflow to extract BES output, feed it into data analysis pipelines and turn
the results into understanding and knowledge. Listing \@ref(code:basic) shows
the R code to achieve it.

Line 31 -- 50 in Listing \@ref(code:basic) shows how to use methods
`tabular_data()`, `read_table()` and `report_data()` in the `EplusSql` class to
extract building area and building energy consumption from standard reports,
zone metadata from standard input and output, and cooling and heating demands
from variable output, with all formatted in a tidy representation. Table
\@ref(tab:mess-csv) and Table \@ref(tab:tidy-csv) gives an example of
EnergyPlus CSV output and results from tidy data extractor from the eplusr R
package for the same variable output, respectively. Table \@ref(tab:tidy-csv)
is the first 10 rows with selected columns of output from line 46 -- 50 in
Listing \@ref(code:basic).

As described in \@ref(sec:eplusr-tidy), one problem when working with EnergyPlus CSV output is that the data is not in a tidy format. In Table
\@ref(tab:mess-csv), one column header was composed in format *Key value*
(`VAV_1`) + *Variable name* (`Air System Total Heating Energy`) + *Units* (`J`) +
*Reporting frequency* (`Hourly`). This often leads to additional data cleaning
efforts. For instance, to get all key values for variable `Air System Total
Heating Energy`, users have to write their methods of splitting column headers
into different parts or subsetting column using regular expressions. Compared to Table
\@ref(tab:mess-csv), the tidy data extractor represented the same underlying
data in a tidy format, as shown in Table \@ref(tab:tidy-csv). It is equivalent
to transform the original column header into four separate variables of
`key_value`, `name`, `units` and `reporting_frequency`. Moreover, instead of
presenting the simulated date and time as strings in Table \@ref(tab:mess-csv),
a time-series column `datetime` in `POSIXct` class was created based on a
derived year value using the algorithm described in Section
\@ref(sec:eplusr-tidy). An additional column named `case` was also
added, which is by default the IDF file name without extension. `case` column
can be quite useful and be served in data analytics as an identifier to
separate each model simulation results when extracting outputs from multiple
parametric simulations. Besides columns currently shown in Table
\@ref(tab:tidy-csv), the extractor also provides a number of additional columns
shown in Fig. \@ref(fig:tidy-extractor), which makes it quite convenient and
straightforward to directly perform further data transformations, including row
filtering and grouped summarization, demonstrated in Line 93 -- 107 in Listing
\@ref(code:basic).

```{r mess-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data/mess.csv")), linesep = "",
    caption = "Example of EnergyPlus CSV output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8, latex_options = "hold_position") %>%
    column_spec(1, "2.5cm")
```

\setlength{\tabcolsep}{1.0pt}
```{r tidy-csv, echo = FALSE, eval = TRUE}
tidy <- data.table::fread(here::here("data/tidy.csv"))
knitr::kable(tidy, linesep = "", align = "c",
    caption = "Tidy format of EnergyPlus output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8, latex_options = "hold_position") %>%
    column_spec(1, "2.0cm") %>%
    column_spec(2, "2.6cm") %>%
    column_spec(3, "1.5cm") %>%
    column_spec(4, "4.6cm") %>%
    column_spec(5, "2.9cm") %>%
    column_spec(6, "0.7cm")
```

Based on the building energy consumption data (line 39 in Listing
\@ref(code:basic)) and the building area (line 36 in Listing
\@ref(code:basic)), the electricity EUI breakdown from various end-use
categories was calculated and a pie chart was created (shown in Fig.
\@ref(fig:eui)) using only 13 lines of codes (line 56 -- 77 in Listing
\@ref(code:basic)). From Fig. \@ref(fig:eui), we can see that most of the
energy has been consumed by interior electric equipment, followed by indoor
lighting, indicating ECMs that help to reduce the plug loads and lighting power
density (LPD) may have a promising on improving the overall energy performance.
We will perform further investigations on this in Section \@ref(sec:param).

```{r eui, echo = FALSE, eval = TRUE, fig.cap = "Annual electricity EUI breakdown", out.width = "50%"}
knitr::include_graphics(here::here("figures/eui.png"))
```

Fig. \@ref(fig:aircon-out) shows the profile of monthly heating and cooling
demands in a unit of $\mathrm{MJ}/\mathrm{m}^2$. It is calculated based on the zone
metadata (line 42 in Listing \@ref(code:basic)) and hourly air system heating
and cooling energy outputs (line 46 -- 50 in Listing \@ref(code:basic)). With
the tidy format and additional metadata columns, these two data fit well in the
data pipeline, making it straightforward and intuitive to perform data
transformation and visualization. Fig. \@ref(fig:aircon-out) is a result of
only around 30 lines of code (line 41 -- 50 and 79 -- 119 in Listing
\@ref(code:basic)). In Fig. \@ref(fig:aircon-out), we can
see that during the transition seasons, including March, April, October and
November, the heating and cooling demands are relatively small compared to
summer and winter seasons, indicating the potential of free-cooling
applications.

```{r aircon-out, echo = FALSE, eval = TRUE, fig.cap = "Monthly heating and cooling demand profile", out.width = "50%"}
knitr::include_graphics(here::here("figures/aircon_out.png"))
```

## Parametric simulation of ECMs on plug loads and LPD {#sec:param}

Since plug loads and interior lighting systems consumed more than 60% of total
electricity, as shown in Fig. \@ref(fig:eui), it is worthwhile investigating
the energy-saving potentials of ECMs on reducing the plug loads and LPD. This
example demonstrates the process to achieve it by parametric simulations using
the parametric prototype in the eplusr framework. Listing \@ref(code:param)
showcases the workflow of utilizing the model API to build measures and reusing
the data analysis code snippets developed in Section \@ref(sec:basic) directly
on the parametric simulation results.

In the original model, the LPD for the office room is 10.76
$\mathrm{W}/\mathrm{m}^2$. Energy savings could be achieved using higher
efficiency lightings, for instance, fluorescent T5 (~7 $\mathrm{W}/\mathrm{m}^2$
LPD), and LED (~5 $\mathrm{W}/\mathrm{m}^2$ LPD). Line 4 -- 14 in Listing
\@ref(code:param) shows a simple measure that modifies the LPD. The core code
is line 14 that assigns all related fields in a whole class to input values,
taking advantage of the flexibly-scoped model API.

Fig. \@ref(fig:sch) shows the schedule profile of lights and plug loads during
weekdays. During the off-work time, we can see that there were still at least
40% plug loads running, indicating the potential to implement optimal
control strategies to turn plug loads off at night to provide energy savings,
without impacting the day-to-day operation of the building occupants. Line 20
-- 40 in Listing \@ref(code:param) shows a measure that modifies the off-work
schedule values of plug loads by multiplying a specified reduction faction
value. This measure aims to demonstrate how objects in an energy model can be
translated into tidy data and how to use the tidy data input API to alter the
model.

```{r sch, eval = TRUE, echo = FALSE, fig.cap = "Schedule profile of lights and plug loads during weekdays", out.width = "50%"}
knitr::include_graphics(here::here("figures/sch.png"))
```

Different measures can be chained together and supplied to the
`apply_measure()` method in the parametric prototype to create parametric
models. As demonstrated in line 42 -- 57 in Listing \@ref(code:param), the
combined measure `ecm` is used to create six models with various combinations
of LPD and plug loads control strategies. Each model is given a specific name.
After the parallel run of simulations (line 67 in Listing \@ref(code:param)),
the tidy data extractor is used to read building energy consumptions of all six
models using one line of code (line 67 in Listing \@ref(code:param)). The
resulting data format is the same as that of a single simulation and is
equivalent to bind rows from six tables into one tidy table with the `case`
column working as an identifier by filling it with names specified in line 56
in Listing \@ref(code:param). This data structure makes it effortless to reuse
most of the EUI breakdown calculation code in Listing \@ref(code:basic), and
perform case filtering (line 80 in Listing \@ref(code:param)), table joins
and grouped summarization (line 84 -- 85 in Listing \@ref(code:param)).

Fig. \@ref(fig:savings) shows the energy savings of various lighting
technologies and plug loads control strategies, based on line 82 -- 96 in
Listing \@ref(code:param). All technologies show overall energy savings to
various degrees. Using higher efficiency lightings shows promising
savings in both reducing the lighting electricity usage, with T5 and LED saving
34.9%% and 53.5% respectively, and the corresponding overall energy savings for
T5 and LED are 7.5% and 11.4%. Strategies of turning off 40% and an 80%
unnecessary plug loads during off-work hours reduce 11.3% and 22.6% electricity
usage from interior equipment and improve the overall energy performance by
3.0% and 5.8%, respectively. Additional energy savings can be obtained when
incorporating LED with 80% reduction factor in off-work plug loads. However,
even the overall energy savings are positive for all cases, the trend for
heating energy shows the opposite. This is due to the reason that all examined
technologies will reduce indoor heat gains which plays a positive role during
heating seasons.

```{r savings, eval = TRUE, echo = FALSE, fig.cap = "Energy savings of various lighting technologies and plug loads control strategies", out.width = "60%"}
knitr::include_graphics(here::here("figures/savings.png"))
```

## Multi-objective optimization using Genetic Algorithm

Automated optimization has become increasingly popular in BES research and
applications to efficiently search and identify optimal or near-optimal design
options meeting one or more key design performance objectives [@Attia2013a].
The epluspar R package implements a `GAOptimJob()` class which is based on the
proposed parametric prototype and the *ecr* R package for a modular framework of
evolutionary computation [@Bossek2017]. It attempts to implement flexible
general-purpose GA interfaces to solve BES-based single- or multi-objective
optimization problems. This example aims to demonstrate the workflow of
performing multi-objective optimization (MOO).

Listing \@ref(code:moo) shows the actual R code for MOO of reducing carbon
emissions and discomfort hours of the medium office reference building at the
same time, by varying indoor heating and cooling setpoint temperatures,
window-to-wall ratio (WWR) and exterior wall insulation thickness. The workflow
shown in Listing \@ref(code:moo) can be divided into four main parts: (1)
create optimization objective functions, (2) specify optimization variables,
(3) set GA operators, and (4) gather results and perform further analyses.

In this example, the two main objectives are reducing carbon emissions and
discomfort hours. Line 11 -- 18 in and line 21--27 Listing \@ref(code:moo)
defined functions to extract the annual total carbon emissions and discomfort
hours counted based on the Standard ASHRAE 55 -- 2004 from the standard reports
using tidy data extractor. Line 30 in Listing \@ref(code:moo) took the two
objective functions and told the algorithm the minimization optimization
direction using the `objective()` method.

Line 36 -- 96 in Listing \@ref(code:moo) defined measure functions to accept
various design options in terms of indoor heating and cooling setpoint,
window-to-wall ratio (WWR) and exterior wall insulation thickness. Line 89
--104 specified the type, range or choices of optimization parameter, using the
similar `apply_measure()` interface. Here we varied heating setpoint
temperature from 18 °C to 22 °C, and cooling setpoint temperature from 23 °C to
27 °C. Both modified with a step of 0.5 °C. The WWR and exterior wall
insulation thickness are continuous variables, with lower and upper bound of
(20%, 80%), and (0.02 m, 0.5 m), respectively. The `GAOptimJob` class provides
a `validate()` method for checking all objective and measure functions by
running a sizing or full simulation with randomly generated design options and
evaluating the fitness to make sure no error occurred.

GA has three key genetic operators: recombinator (also called crossover),
mutator, and selector, providing detailed procedures and steps on how to
generate children from parent solutions. Line 114 -- 118 in Listing
\@ref(code:moo) directly specified those three operators with the default
values that the `GAOptimJob` class provided and tweaked to directly perform MOO
using the Non-Dominated Sorting Genetic Algorithm (NSGA-II). The `terminator()`
method was used to specify the conditions to terminate the computation. In this
example, we set it to stop when one hundred generations were evaluated.

With all objectives, variables and operators specified, the optimization
started with twenty individuals per generation, resulting in a total of two
thousand annual energy simulations. Line 126 and 129 in Listing
\@ref(code:moo) extracted all population and Pareto font into two tidy tables
for future analyses, using the `population()` and `pareto_set()` method. Fig.
\@ref(fig:pareto) shows the Pareto front of discomfort hours and total carbon
emissions, generated using line 132 -- 139 in Listing \@ref(code:moo). The
final Pareto font contained 20 unique solutions.

```{r pareto, echo = FALSE, eval = TRUE, fig.cap = "Pareto front of discomfort hours and carbon emissions", out.width = "40%"}
knitr::include_graphics(here::here("figures/pareto.png"))
```

Fig. \@ref(fig:parallel) shows the parallel coordinates charts of the Pareto
set. The carbon emissions have seen a significant reduction from the original
value of 290ton. However, there were 10 out of 20 solutions in the Pareto set
that performed worse in terms of providing a satisfactory indoor thermal
environment. One possible solution to avoid this is to add a constraint when
evaluating the fitness of the `discomfort_hours` objective, making sure all
solutions that have larger discomfort hours should be abandoned.

```{r parallel, echo = FALSE, eval = TRUE, fig.cap = "Parallel coordinates chart of the Pareto set", out.width = "70%"}
knitr::include_graphics(here::here("figures/parallel.png"))
```

## Model calibration using Bayesian theory

Model calibration is an essential process to achieve greater confidence in BES
results. In recent years there has been an increasing application of Bayesian
approaches for BES calibration [@Chong2017]. Bayesian calibration is carried
out following the statistical formulation proposed by Kennedy and O'Hagan
[@Chong2018]. This example aims to demonstrate the model calibration workflow
using the epluspar R package. The epluspar R package implements the Bayesian
calibration algorithm proposed by @Chong2017 and guidelines proposed by
@Chong2018, and encapsulates it into the `BayesCalibJob` class, which is based
on the parametric prototype. Listing \@ref(code:bc) shows the workflow of
calibrating one VAV fan total efficiency in the medium office reference model
using observed fan input air flow rate and electrical power.

The initial step for the Bayesian calibration is to collect data for observable
input and output, and weather conditions. Since the reference model represents
a virtual building with no measured data, we created some synthetic data of the
examined period of July 1st to July 3rd using simulations (line 1 -- 27 in Listing
\@ref(code:bc)). Also, the TMY3 weather data was used, instead of the Actual
Meteorological Year (AMY) weather data. For real practice, the actual
measurable variables may not be directly representable in EnergyPlus. So the
first step may also include a mapping process to transform measured variables
into EnergyPlus output variables and connect the transformed measured values
with the model using schedule files or other techniques.

The next step was to specify the observable input and output variables using
the `input()` and `output()` methods in `BayesCalibJob` class (line 29 -- 39 in
Listing \@ref(code:bc)). The calibration parameters, together with the number
of EnergyPlus simulations was described using the `param()` method (line 41 --
45 in Listing \@ref(code:bc)). Each calibration parameter was given a lower and
upper bound value. Following the Bayesian calibration guidelines [@Chong2018],
the epluspar R package also introduces a `SensitivityJob` class based on the
parametric prototype to perform calibration parameter screening with the Morris
method. Once the calibration parameters were given, the `samples()` method in
`BayesCalibJob` class will use the Latin Hypercube Sampling (LHS) algorithm to
generate parameter sample values based on the lower and upper bound and the
simulation number specified (line 47 -- 48 in Listing \@ref(code:bc)). The
benefit of LHS is that it will try to cover as much as possible in the
multi-dimensional space of the calibration parameters. After all simulations
completed (line 50 -- 51 in Listing \@ref(code:bc)), the `data_sim()` method
gathered all data of simulated output variables and aggregated them into the
same time frequency as the actual measured data (line 53 -- 54 in Listing
\@ref(code:bc)). With all data required specified (line 56 -- 60 in Listing
\@ref(code:bc)), the `stan_run()` method was used to employ the Bayesian
calibration algorithm written in probabilistic programming language Stan (line
62 -- 63 in Listing \@ref(code:bc)). Once completed, the posterior
distributions of calibration parameters, predicted output variable values and
uncertainty statistical indicators including Normalized Mean Biased Error
(NMBE) and Coefficient of Variation of the Root Mean Squared Error (CVRMSE) can
be retrieved using method `post_dist()`, `prediction()` and `evaluate()`,
respectively.

Fig. \@ref(fig:dist-post) gives a density plot showing the posterior
distributions of calibrated fan total efficiency, created using line 74 -- 79
in Listing \@ref(code:bc). The mean value of the posterior distribution was
0.60, which is quite close to the actual value of 0.5915 specified in the
original model. Fig. \@ref(fig:uncertainties) gives a box plot showing the
distribution of CVRMSE and NMBE per Markov Chain Monte Carlo (MCMC) sampling,
created using line 81 -- 85 in Listing \@ref(code:bc). The mean NMBE value was
quite close to zero and the averaged CVRMSE is around 3.0%. Both values met the
thresholds of CVRMSE $\leq$ 15% and NMBE $\leq$ 5% set by ASHRAE [@ASHRAE2014].
The satisfactory results are expected since we use synthetic data. However,
the overall workflow shown in this example can be applied to Bayesian
calibration on building energy models with real measured data.

```{r dist-post, echo = FALSE, eval = TRUE, fig.cap = "Posterior distribution of fan total efficiency", out.width = "40%"}
knitr::include_graphics(here("figures/dist_post.png"))
```

```{r uncertainties, echo = FALSE, eval = TRUE, fig.cap = "Distribution of CVRMSE and NMBE per MCMC sample", out.width = "40%"}
knitr::include_graphics(here("figures/uncertainties.png"))
```

# Conclusion

Building energy simulation (BES) has been widely adopted for the investigation of
environmental and energy performance for different design and retrofit
alternatives. The absence of seamless integration of BES and data-centric
analysis raises problems in both the productivity and also the credibility of
BES studies. This paper proposed a novel holistic framework called 'eplusr' to
bridge the gap between the building energy simulation and data science domains.

Eplusr differs from existing frameworks by its data-centric design philosophy.
It provides a tidy data interface for BES that matches the semantics of the
simulation results with the data representation. The tidy data extractor
provides the possibilities to query BES outputs with various types of
specifications, which makes it easy and quite straightforward to get simulation
results of any specified time, units and variables in a consistent manner. The
tidy-formatted results can be easily fed to various data-centric analytics
using existing tools in R.

The parametric prototype developed in this framework provides a set of
abstractions to ease the process of parametric model generation, design
alternative evaluation and large parametric simulation management. It is
capable of defining various analyses using any algorithms available in R. The
flexibility and extensibility of the parametric simulation prototype in this
framework are demonstrated by its easy adoption to perform multi-objective
optimization and Bayesian calibration.

The need for reproducibility in BEM is growing significantly together with the
ongoing trend of the increasing complexity of BEM projects. The eplusr framework
provides a possible solution for this by developing a portable and reusable BES
computation environment based on Docker containerization, encapsulating the
toolchains for statistical computing, building energy modeling, data analytics
and literate programming.

# Limitations and future work

The main limitation of the proposed framework lies in its R-oriented workflows,
which currently may not be widely adopted in industry fields. Prospective users
of the framework who do not know R must spend time learning how to use it. This
drawback may be compensated by the growing user community.

Since the eplusr framework is mainly focused on modifying existing BES models,
instead of creating new ones from scratch, currently it has limited capacities
to perform sophisticated geometry transformation including surface matching and
rotation. Operations such as creating or replacing one whole HVAC system may
also be time-consuming processes.

# Supplementary materials {-}

The supplementary files include code and datasets used in this article. More
is available at https://github.com/ideas-lab-nus/eplusr-paper.

# Acknowledgements {-}

This research was funded by the Republic of Singapore's National Research
Foundation through a grant to the Berkeley Education Alliance for Research in
Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and
Sustainability in the Tropics (SinBerBEST) Program. BEARS has been established
by the University of California, Berkeley as a center for intellectual
excellence in research and education in Singapore.

# References {#references .unnumbered}

<div id="refs"></div>

\clearpage

`\begin{appendices}`{=latex}

\renewcommand{\thesection}{\Alph{section}.}

# Code for data exploration on the EUI and the heating and cooling demands

```{r basic, echo = TRUE, eval = run_sim, code.cap = "Data exploration on the EUI and the heating and cooling demands"}
# load package
library(eplusr)
library(tidyverse) # for data-driven analytics

# get EnergyPlus v9.1 installation directory
dir <- eplus_config(9.1)$dir

# use example model and weather file distributed with EnergyPlus v9.1
path_model <- file.path(dir, "ExampleFiles/RefBldgMediumOfficeNew2004_Chicago.idf")
path_weather <- file.path(dir, "WeatherData/USA_IL_Chicago-OHare.Intl.AP.725300_TMY3.epw")

# read model
idf <- read_idf(path_model)

#############
# Model API #
#############

# make sure weather file input is respected
idf$SimulationControl$Run_Simulation_for_Weather_File_Run_Periods <- "Yes"

# make sure energy consumption is presented in kWh
idf$OutputControl_Table_Style$Unit_Conversion <- "JtoKWH"

# save the modified model into a temporary folder
idf$save(file.path(tempdir(), "MediumOffice.idf"))

# run annual simulation
job <- idf$run(path_weather)

#######################
# Tidy data extractor #
#######################

# read building area from Standard Reports
area <- job$tabular_data(table_name = "Building Area", wide = TRUE)[[1L]]

# read building energy consumption from Standard Reports
end_use <- job$tabular_data(table_name = "End Uses", wide = TRUE)[[1L]]

# read zone metadata from Standard Input and Output
zones <- job$read_table("Zones")

# read hourly air-conditioning system output with all additional metadata for
# the annual simulation from Variable Output
aircon_out <- job$report_data(
    name = sprintf("air system total %s energy", c("heating", "cooling")),
    environment_name = "annual",
    all = TRUE
)

#########################
# Data-driven analytics #
#########################

# calculate Energy Use Intensity (EUI) for electricity
eui <- end_use %>%
    # only select columns of interest
    select(category = row_name, electricity = `Electricity [kWh]`) %>%
    # get rid of category with empty energy consumption
    filter(electricity > 0.0) %>%
    # order by value
    arrange(-electricity) %>%
    # calculate EUI
    mutate(eui = round(electricity / area$'Area [m2]'[1], digits = 2)) %>%
    # calculate proportion of each category
    mutate(proportion = round(eui / eui[1] * 100, digits = 2)) %>%
    # remove electricity column
    select(-electricity)

# plot a pie chart to show EUI breakdown
p_eui <- eui %>%
    filter(category != "Total End Uses") %>%
    mutate(category = as_factor(sprintf("%s [%.2f%%]", category, proportion, "%"))) %>%
    ggplot(aes("", proportion, fill = category)) +
    geom_bar(stat = "identity", width = 1, color = "black", size = 0.2) +
    coord_polar("y", start = 0)

# calculate air-conditioned floor area per storey
storey <- zones %>%
    # exclude plenum zones
    filter(is_part_of_total_area == 1) %>%
    # group by centroid height
    group_by(centroid_height = round(centroid_z, digits = 4)) %>%
    # calculate total floor area
    summarise(floor_area = sum(floor_area)) %>%
    ungroup() %>%
    # add storey index
    arrange(centroid_height) %>%
    mutate(storey = seq_len(n()), air_system = paste("VAV", storey, sep = "_")) %>%
    select(air_system, floor_area)

# get monthly heating and cooling demands per served area
aircon_out_mon <- aircon_out %>%
    # only consider weekdays
    filter(!day_type %in% c("Holiday", "Saturday", "Sunday")) %>%
    # add an identifier column to indicate cooling and heating condition
    mutate(type = case_when(
        str_detect(name, "Heating") ~ "Heating",
        str_detect(name, "Cooling") ~ "Cooling"
    )) %>%
    # add floor area served by each air-conditioning system
    left_join(storey, c("key_value" = "air_system")) %>%
    # calculate the monthly averaged heating and cooling demands in MJ/m2
    group_by(month, type, air_system = key_value) %>%
    summarise(system_output = sum(value) / 1e6 / floor_area[1]) %>%
    ungroup()

# plot a pie chart to show the heating and cooling demand profile
p_aircon_out <- aircon_out_mon %>%
    mutate(month = as.factor(month)) %>%
    mutate(system_output = case_when(
        type == "Heating" ~ -system_output,
        type == "Cooling" ~ system_output
    )) %>%
    ggplot() +
    geom_col(aes(month, system_output, group = type, fill = type), position = "dodge") +
    facet_wrap(vars(air_system), ncol = 1) +
    labs(x = "", y = "Heating and cooling demands / MJ m-2")
```

```{r basic-post-process, echo = FALSE, eval = run_sim}
tidy <- aircon_out[order(key_value, -name), .(case, datetime, key_value, name, reporting_frequency, units, value)][1:10]
data.table::fwrite(tidy, here("data/tidy.csv"), dateTimeAs = "write.csv")

mess <- job$report_data(
    key_value = "vav_1", name = "air system total heating energy",
    wide = TRUE, month = 1, day = 1, hour = 1:10)[, -"case"]
data.table::fwrite(mess, here("data/mess.csv"))

# colorblind-friendly palette
pal_cb <- c(
    "#999999", #1
    "#E69F00", #2
    "#CC79A7", #3
    "#56B4E9", #4
    "#009E73", #5
    "#F0E442", #6
    "#0072B2", #7
    "#D55E00"  #8
)

p_eui_post <- p_eui +
    theme_void() +
    scale_fill_manual(values = pal_cb) +
    theme(plot.margin = unit(c(-1.0, 0, -1.0, -1.0), "cm"))
ggsave(here("figures/eui.png"), p_eui_post, width = 6, height = 3.5, dpi = 600)

p_aircon_out_post <- p_aircon_out +
    scale_fill_manual(values = pal_cb[c(4, 3)])
ggsave(here("figures/aircon_out.png"), p_aircon_out_post, width = 6, height = 6, dpi = 600)
```

# Code for parametric simulation of ECMs on plug loads and LPD

```{r plot-sch, echo = FALSE, eval = run_sim}
# extract the schedule of plug loads during weekdays
sch_plug <- idf$
    ElectricEquipment$
    Core_bottom_PlugMisc_Equip$
    ref_to_object("Schedule Name")[[1]]$
    to_table()[5:20]

sch_plug <- data.table::data.table(
    hour = sch_plug[1:.N %% 2 == 1, as.integer(str_extract(value, "\\d+"))],
    val  = sch_plug[1:.N %% 2 == 0, as.double(value)])[
    J(1:24), on = "hour", roll = -Inf]

sch_light <- idf$Lights$
    Core_bottom_Lights$
    ref_to_object("Schedule Name")[[1]]$
    to_table()[5:22]

sch_light <- data.table::data.table(
    hour = sch_light[1:.N %% 2 == 1, as.integer(str_extract(value, "\\d+"))],
    val  = sch_light[1:.N %% 2 == 0, as.double(value)])[
    J(1:24), on = "hour", roll = -Inf]

sch <- data.table::rbindlist(list(
    sch_plug[, type := "Plug laods"], sch_light[, type := "Lights"]
))

p_sch <- sch %>%
    ggplot() +
    geom_line(aes(hour, val, color = type), size = 1) +
    scale_x_continuous(NULL, breaks = seq(1, 24, 2), expand = c(0, 0)) +
    scale_y_continuous("Schedule value", breaks = seq(0, 1, 0.2), expand = c(0, 0)) +
    scale_color_manual(values = pal_cb[c(2, 5)]) +
    guides(color = guide_legend(NULL)) +
    coord_cartesian(ylim = c(0, 1)) +
    theme_classic() +
    theme(
        legend.position = "bottom",
        panel.grid.major = element_line(color = "grey95")
    )
ggsave(here::here("figures/sch.png"), p_sch, width = 6, height = 3, dpi = 600)
```

```{r param, echo = TRUE, eval = run_sim, code.cap = "Parametric simulation of ECMs on plug loads and LPD"}
# create a parametric prototype of given model and weather file
param <- param_job(idf, path_weather)

#####################
#  Create Measures  #
#####################

# create a measure for modifying LPD
set_lpd <- function (idf, lpd = NA) {
    # keep the original if applicable
    if (is.na(lpd)) return(idf)

    # set 'Watts per Zone Floor Area' in all 'Lights' objects as input LPD
    idf$set(Lights := list(watts_per_zone_floor_area = lpd))

    # return the modified model
    idf
}

# create a measure for reducing plug loads during off-work time
set_nightplug <- function (idf, frac = NA) {
    # keep the original if applicable
    if (is.na(frac)) return(idf)

    # extract the plug load schedule into a tidy table
    sch <- idf$to_table("bldg_equip_sch")

    # modify certain schedule value specified using field names
    sch <- sch %>%
        mutate(value = case_when(
            field %in% paste("Field", c(4,14,16,18)) ~ sprintf("%.2f", as.numeric(value) * frac),
            TRUE ~ value
        ))

    # update schedule object using the tidy table
    idf$update(sch)

    # return the modified model
    idf
}

# combine two measures into one
ecm <- function (idf, lpd, nightplug_frac) {
    idf %>% set_lpd(lpd) %>% set_nightplug(nightplug_frac)
}

####################
#  Apply Measures  #
####################

# apply measures and create parametric models
param$apply_measure(ecm,
                  lpd = c(   NA,  7.0,   5.0,        NA,        NA,           5.0),
       nightplug_frac = c(   NA,   NA,    NA,       0.6,       0.2,           0.2),
               # name of each case
               .names = c("Ori", "T5", "LED", "0.6Frac", "0.2Frac", "LED+0.2Frac")
)

# run parametric simulations in parallel
param$run()

#########################
# Data-driven analytics #
#########################

# read building energy consumption from Standard Reports
param_end_use <- param$tabular_data(table_name = "End Uses", wide = TRUE)[[1L]]

# calculate EUI breakdown
param_eui <- param_end_use %>%
    select(case, category = row_name, electricity = `Electricity [kWh]`) %>%
    filter(electricity > 0.0) %>%
    arrange(-electricity) %>%
    mutate(eui = round(electricity / area$'Area [m2]'[1], digits = 2)) %>%
    select(case, category, eui) %>%
    # exclude categories that did not change
    filter(category != "Pumps", category != "Exterior Lighting")

# extract the seed model, i.e. "Ori" case as the baseline
ori_eui <- param_eui %>% filter(case == "Ori") %>% select(-case)

# calculate energy savings based on the baseline EUI
param_savings <- param_eui %>%
    right_join(ori_eui, by = "category", suffix = c("", "_ori")) %>%
    mutate(savings = (eui_ori - eui) / eui_ori * 100) %>%
    filter(case != "Ori")

# plot a bar chart to show the energy savings
p_param_savings <- param_savings %>%
    mutate(case = factor(case, names(param$models()))) %>%
    ggplot(aes(case, savings, fill = category)) +
    geom_bar(position = "dodge", stat = "identity", width = 0.6, color = "black",
        show.legend = FALSE) +
    facet_wrap(vars(category), nrow = 2) +
    labs(x = NULL, y = "Energy savings / %") +
    coord_flip()
```

```{r param-post-process, echo = FALSE, eval = run_sim}
p_param_savings_post <- p_param_savings +
    scale_fill_manual(values = pal_cb[c(7, 6, 3, 1, 8, 2)])
ggsave(here("figures/savings.png"), p_param_savings_post, width = 6, height = 3, dpi = 600)
```

# Code for multi-optimization using Genetic Algorithm

```{r moo, echo = TRUE, eval = run_sim, code.cap = "Multi-objective optimization using Genetic Algorithm"}
# load package
library(epluspar)

# create a GA optimization job
ga <- gaoptim_job(idf, path_weather)

############################
# Optimization objectives  #
############################

# define an objective function to get carbon emissions
carbon_emissions <- function (idf) {
    as.double(idf$last_job()$tabular_data(
        report_name = "emissions data summary",
        row_name = "Annual Sum or average",
        column_name = "carbon equivalent:facility"
    )$value)
}

# define an objective function to get discomfort hours
discomfort_hours <- function (idf) {
    as.double(idf$last_job()$tabular_data(
        table_name = "comfort and setpoint not met summary",
        row_name = "time not comfortable based on simple ASHRAE 55-2004",
        column_name = "facility"
    )$value)
}

# set optimization objectives
ga$objective(carbon_emissions, discomfort_hours, .dir = "min")

###########################
# Optimization variables  #
###########################

# define a measure to change heating setpoint
set_heating_setpoint <- function (idf, sp) {
    sp <- as.character(sp)
    idf$set(htgsetp_sch = list(field_6 = sp, field_16 = sp, field_21 = sp))
    idf
}

# define a measure to change cooling setpoint
set_cooling_setpoint <- function (idf, sp) {
    sp <- as.character(sp)
    idf$set(clgsetp_sch = list(field_6 = sp, field_13 = sp))
    idf
}

# define a measure to change the window-to-wall ratio
set_wwr <- function (idf, wwr) {
    # extract data of all windows
    win <- idf$to_table(class = "FenestrationSurface:Detailed", wide = TRUE, string_value = FALSE)

    # extract data of all parent walls
    wall <- idf$to_table(win[["Building Surface Name"]], wide = TRUE,
        string_value = FALSE, group_ext = "index"
    )

    # calculate new X and Y coordinates for windows
    cols <- sprintf("Vertex %s-coordinate", c("X", "Y", "Z"))
    ratio <- c(0.999, 0.999, wwr)
    cal_coords <- function (coords, ratio) {
        list(round((coords[[1]] - mean(coords[[1L]])) * ratio + mean(coords[[1L]]), 3))
    }
    wall[, .SDcols = cols, by = "id", c(cols) := mapply(
        cal_coords, coords = .SD, ratio = ratio, SIMPLIFY = FALSE
    )]

    # update coordinates of windows
    coords <- wall[, lapply(.SD, unlist), .SDcols = cols, by = "id"]
    coords <- lapply(coords[, -"id"], function (x) as.data.frame(t(matrix(x, nrow = 4))))
    for (axis in c("X", "Y", "Z")) {
        cols <- sprintf("Vertex %i %s-coordinate", 1:4, axis)
        win[, c(cols) := coords[[sprintf("Vertex %s-coordinate", axis)]]]
    }

    idf$update(dt_to_load(win))

    idf
}

# define a measure to change the insulation thickness of the exterior wall
set_insulation <- function (idf, thickness) {
    idf$set(`Steel Frame NonRes Wall Insulation` = list(thickness = thickness))
    idf
}

# combine all measures into one
design_options <- function (idf, htg_sp, clg_sp, wwr, insulation_thickness) {
    idf <- set_heating_setpoint(idf, htg_sp)
    idf <- set_cooling_setpoint(idf, clg_sp)
    idf <- set_wwr(idf, wwr)
    idf <- set_insulation(idf, insulation_thickness)
    idf
}

# specify design space of parameters
ga$apply_measure(design_options,
    htg_sp = choice_space(seq(18, 22, 0.5)),
    clg_sp = choice_space(seq(23, 27, 0.5)),
    wwr = float_space(0.2, 0.8),
    insulation_thickness = float_space(0.02, 0.5)
)

# validate to make sure all measures and objective functions work properly
ga$validate(ddy_only = FALSE)

#################
# GA operators  #
#################

# specify how to mix solutions
ga$recombinator()
# specify how to change parts of one solution randomly
ga$mutator()
# specify how to select best solutions
ga$selector()
# specify the conditions when to terminate the computation
ga$terminator(max_gen = 100L)

# run optimization
ga$run(mu = 20)

# get all population
population <- ga$population()

# get Pareto set
pareto <- ga$pareto_set()

# plot Pareto front
p_pareto <- ggplot() +
    geom_point(aes(carbon_emissions, discomfort_hours), population, color = "darkgoldenrod", alpha = 0.5) +
    geom_line(aes(carbon_emissions, discomfort_hours), pareto, color = "darkblue", linetype = 2) +
    geom_point(aes(carbon_emissions, discomfort_hours), pareto, color = "darkblue", size = 2) +
    scale_x_continuous("Carbon emissions / ton", labels = scales::number_format(scale = 0.001)) +
    scale_y_continuous("Discomfort time based on\nsimple ASHRAE 55-2004 / Hours",
        labels = scales::number_format(big.mark = ",")
    )
```

```{r moo-post-process, echo = FALSE, eval = run_sim}
pareto <- data.table::fwrite(pareto, here::here("data/poreto.csv"))
population <- data.table::fwrite(pareto, here::here("data/population.csv"))

ggsave(here::here("figures/pareto.png"), p_pareto, width = 6, height = 5, dpi = 600)

rng <- tribble(
    ~name                  , ~min        , ~max        ,
    "htg_sp"               , "18 °C"     , "22 °C"     ,
    "clg_sp"               , "23 °C"     , "27 °C"     ,
    "wwr"                  , "20 %"      , "80 %"      ,
    "insulation_thickness" , "0.02 m"    , "0.50 m"    ,
    "carbon_emissions"     , "215.5 ton" , "246.6 ton" ,
    "discomfort_hours"     , "648 hours" , "3507 hour"
)

normalize <- function (x, min = NULL, max = NULL) {
    if (is.null(min)) min <- min(x)
    if (is.null(max)) max <- max(x)
    (x - min) / (max - min)
}

p_parallel <- pareto %>%
    mutate(
        htg_sp = normalize(htg_sp, 18, 22),
        clg_sp = normalize(clg_sp, 23, 27),
        wwr = normalize(wwr, 0.2, 0.8),
        insulation_thickness = normalize(insulation_thickness, 0.02, 0.5),
        carbon_emissions = normalize(carbon_emissions),
        discomfort_hours = normalize(discomfort_hours)
    ) %>%
    pivot_longer(-index) %>%
    mutate(index = as_factor(index), name = as_factor(name)) %>%
    ggplot(aes(name, value, group = index, color = index)) +
    geom_segment(aes(x = name, xend = name, y = 0.0, yend = 1.0), color = "grey80") +
    geom_hline(aes(yintercept = 1.0), color = "grey80") +
    geom_hline(aes(yintercept = 0.0), color = "grey80") +
    geom_line(show.legend = FALSE, alpha = 0.5, size = 1.2) +
    geom_point(show.legend = FALSE, alpha = 0.5, size = 2.5) +
    geom_text(aes(name, 1, label = max), rng, vjust =-1.0, inherit.aes = FALSE, color = "grey50", fontface = "bold") +
    geom_text(aes(name, 0, label = min), rng, vjust = 2.0, inherit.aes = FALSE, color = "grey50", fontface = "bold") +
    scale_x_discrete(NULL, position = "top", expand = expand_scale(0.05, 0.05),
        labels = c("Heating\nSetpoint", "Cooling\nSetpoint", "WWR", "Insulation\nThickness", "Carbon\n Emissions", "Discomfort\nHours")
    ) +
    scale_y_continuous(expand = expand_scale(mult = c(0.05, 0.08))) +
    scale_color_viridis_d() +
    theme(
        axis.text.x = element_text(size = 12, vjust = -2.0, face = "bold", color = "grey30"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = NA)
    )

ggsave(here("figures/parallel.png"), p_parallel, width = 10, height = 6, dpi = 600)
```

# Code for model calibration using Bayesian theory

```{r bc, echo = TRUE, eval = run_sim, code.cap = "Bayesian calibration"}
################################################################################
# NOTE: for demonstration, we use the seed model to generate some synthetic data
# clone the original model
tmp <- idf$clone()
# remove all existing run periods
tmp$RunPeriod <- NULL
# add a new run period from Jul 1st to Jul 3rd
tmp$add(RunPeriod = list("test", 7, 1, NULL, 7, 3))
# add variables of interest to output
tmp$add(Output_Variable = list("VAV_1_Fan", "Fan Electric Power", "Hourly"))
# get rid of design day variable output data
tmp$SimulationControl$set(run_simulation_for_sizing_periods = "No")
# save the model to a temporary file
tmp$save(tempfile(fileext = ".idf"))
# run simulation
job <- tmp$run(path_weather)
# extract fan electric power in 6-hourly frequency
fan_power <- job$report_data(name = bc$output()$variable_name, all = TRUE) %>%
    epluspar:::report_dt_aggregate("6 hour") %>%
    eplusr:::report_dt_to_wide()
# insert Gaussian noise
fan_power <- fan_power %>%
    select(-`Date/Time`) %>%
    rename(power = everything()) %>%
    mutate(power = power + rnorm(length(power), sd = 0.05 * sd(power))) %>%
    mutate(power = case_when(power < 0.0 ~ 0.0, TRUE ~ power))
################################################################################

# load library
library(epluspar)

# create a `BayesCalibJob` object:
bc <- bayes_job(idf, path_weather)

# specify parameters that can be measured
bc$input("VAV_1 Supply Equipment Outlet Node", "System Node Mass Flow Rate", "Hourly")

# specify the parameter to predict
bc$output("VAV_1_Fan", "Fan Electric Power", "Hourly")

# specify parameters to calibrate
bc$param(
    VAV_1_Fan = list(fan_total_efficiency = c(0.4, 0.8)),
    .num_sim = 30, .names = "FanEfficiency"
)

# get sample parameter values generated using Latin Hypercube Sampling (LHS)
bc$samples()

# run simulations from Jul 1st to Jul 3rd
bc$eplus_run(run_period = list("example", 7, 1, NULL, 7, 3))

# gather simulated data in 6-hour time frequency
bc$data_sim("6 hour")

# set field data
bc$data_field(fan_power)

# get input data for Stan
bc$data_bc()

# run Bayesian calibration using Stan
res <- bc$stan_run(iter = 300, chains = 3)

# extract posterior distributions of calibration parameter
dist <- bc$post_dist()

# extract prediction values
pred <- bc$prediction()

# evaluate the uncertainties including NMBE and CV(RMSE)
uncert <- bc$evaluate()

# draw a density plot for the posterior distributions of calibration parameters
p_dist <- dist %>%
    pivot_longer(-sample) %>%
    ggplot() +
    geom_density(aes(value, fill = name), alpha = 0.5) +
    geom_vline(aes(xintercept = mean(value)), linetype = 2, size = 1)

# draw a boxplot to show the distributions of uncertainty indices
p_uncert <- uncert %>%
    pivot_longer(-sample) %>%
    ggplot() +
    geom_boxplot(aes(name, value, fill = name), alpha = 0.5)
```

```{r bc-post-process, echo = FALSE, eval = run_sim}
p_dist_post <- p_dist +
    scale_x_continuous("Fan total efficiency") +
    geom_text(aes(x = mean(value) * 1.001, y = 80, label = paste0("Mean:", scales::number(mean(value), 0.01))), hjust = -0.2) +
    scale_fill_manual(values = pal_cb[3]) +
    theme(legend.position = "none")
ggsave(here("figures/dist_post.png"), p_dist_post, width = 6, height = 3.5, dpi = 600)

p_uncert_post <- p_uncert +
    scale_x_discrete(NULL, labels = c("CV(RMSE)", "NMBE")) +
    scale_y_continuous(NULL, labels = scales::percent_format(0.01)) +
    scale_fill_manual(values = pal_cb[c(4, 5)]) +
    theme(
        legend.position = "none",
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)
    )
ggsave(here("figures/uncertainties.png"), p_uncert_post, width = 6, height = 3, dpi = 600)
```

`\end{appendices}`{=latex}
