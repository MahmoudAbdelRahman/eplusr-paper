---
title: A framework for integrating building energy simulation and data-driven analytics
author:
  - name: Hongyuan Jia
    email: hongyuan.jia@bears-berkeley.sg
    affiliation: SinBerBEST
  - name: Adrian Chong
    email: adrian.chong@nus.edu.sg
    affiliation: NUS
    footnote: 1
address:
  - code: SinBerBEST
    address: |
      SinBerBEST Program, Berkeley Education Alliance for Research in Singapore,
      Singapore, 138602, Singapore
  - code: NUS
    address: |
      Department of Building, School of Design and Environment, National
      University of Singapore, 4 Architecture Drive, Singapore, 117566,
      Singapore
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  Building energy simulation (BES) has been widely adopted for the
  investigation of environmental and energy performance for different design
  and retrofit alternatives. In BES, data-centric analytics is of commensurate
  importance in order to turn BES results into understanding, insight, and
  knowledge. However, there is no widely adopted solution to provide seamless
  integration of BES and data-centric analytics. This paper presents a newly
  developed framework 'eplusr' for conducting parametric analysis using
  EnergyPlus via the R programming language. With a data-centric design
  philosophy, the proposed framework focuses on better and more seamless
  integration between BES and data-driven analytics. It provides a Tidy data
  format for BES that matches the semantics of the simulation results which can
  be easily fed to various data analytics workflows in R. The framework also
  provides an infrastructure to bring portable and reusable computation
  environment for building energy modeling using, which aims to facilitate the
  reproducibility research in building energy domain. This paper discusses the
  philosophy behind the framework, its architecture and core capabilities, and
  demonstrates the streamlined workflow of using this framework to conduct data
  exploration, parametric simulation, multi-objective optimization, and
  Bayesian calibration.
journal: "Automation in Construction"
date: "`r Sys.Date()`"
bibliography: references.bib
layout: 3p, times
colorlinks: yes
link-citations: yes
linenumbers: false
output:
  bookdown::pdf_book:
    includes:
      in_header: header.tex
    base_format: rticles::elsevier_article
    keep_tex: yes
---

```{r setup, include = FALSE}
library(eplusr)
library(kableExtra)
library(here)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  eval = FALSE,
  comment = "#>",
  out.width = "\\columnwidth",
  fig.path = "../figures/",
  fig.pos="ht"
)

# code chunk cross-ref
Chunk <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function (x, options) {
    x <- Chunk(x, options)
    if (is.null(options$label)) return(x)
    if (!options$echo) return(x)
    if (!is.null(options$fig.cap)) return(x)
    # use chunk label as cross-ref label
    x <- paste0(
        # use Pandoc raw-attribute to preserve raw LaTeX content
        # https://pandoc.org/MANUAL.html#extension-raw_attribute
        "`\\begin{Shaded}`{=latex}\n",
        x, "\n",
        "\\captionof{code}{", options$code.cap, "\\label{code:", options$label, "}}\n",
        "`\\end{Shaded}`{=latex}\n"
    )
    x
})
```

# Highlights {.unnumbered}

1. A framework integrates EnergyPlus with data science through R
2. The framework includes a parametric simulation  with flexibility and extensibility
3. Tidy data format is introduced to facilitate data-driven analytics of BES research
4. Docker together with R project management can be adopted to bring reproducible BESs

# Introduction

Building energy simulation (BES) is increasingly being used for the analysis and
prediction of building energy consumption, measurement and verification and the
evaluation of energy conservation measures (ECMs) [@Chong2017]. It has played a
growing role in the design and operation of low energy, high-performance
buildings and development of policies that drive the achievement of reducing
energy use and greenhouse-gas emissions in the buildings sector [@Hong2018].

BES offers an alternative approach that encourages customized, integrated design
solutions and the development of BES tools has been pronounced over the decades
[@Hong2000; @Hong2018]. The core tools in the BES field are the whole-building
energy simulation programs which provide users with key building performance
indicators such as energy use and demand, temperature, humidity, and costs
[@Crawley2008a]. Among them, EnergyPlus is one of the widely used whole
building energy simulation tools [@Crawley2001] and has been adopted as a core
component or a building simulation engine of energy performance assessment in
both free and commercial building design and analysis tools [@See2011; @DesignBuilderSoftwareLtd2020; @Guglielmetti2011; @Yi2020; @Roudsari2013].

Parametric analysis in BES has been a powerful approach to enable investigation
of environmental and energy performance for different design and retrofit
alternatives. Choosing the appropriate combination of design options is
a complex task that requires the management of a large amount of information
on the properties of design options and the simulation of their performance
[@Purup2020]. Parametric analysis involves tedious file management tasks,
repeated entry of model parameters, the application of design transformations
and the execution of large-scale analyses [@Macumber2012], which can be
time-consuming and error-prone. Simulation task automation has been approved to
be an extremely useful way to reduce human intervention and improve the
efficiency of large parametric analysis. Multiple efforts have been made with
this regard and there are several existing EnergyPlus-based parametric
simulation tools developed as shown in Table \@ref(tab:pat-sum). Some tools consist of graphical user interfaces
(GUIs) built on EnergyPlus while some use general-purpose scripting languages
with a full complement of programming features and libraries [@Roth2018].

```{r pat-sum, eval = TRUE, include = TRUE, echo = FALSE}
compare <- data.table::fread(here::here("data-raw/compare.csv"))
compare %>%
    knitr::kable(format = "latex", caption = "Comparison of parametric simulation framework based on EnergyPlus") %>%
    kableExtra::kable_styling(full_width = TRUE)
```

OpenStudio [@Guglielmetti2011] is a free open-source software toolkit designed
for energy modeling and can be used to efficiently create or modify models,
manage individual or multiple simulations, and visualize results. OpenStudio
has its own format (`.OSM`) and schema for EnergyPlus model representation
which will eventually be translated into EnergyPlus models. Parametric Analysis
Tools (PAT) a GUI application that is part of OpenStudio toolkit. It aims to
enable customizable and shareable parametric descriptions of ECMs
[@Parker2014]. It leverages OpenStudio Measures which are reusable script
written in Ruby language to manipulate OpenStudio models can be used to compare
manually specified combinations of measures, optimize designs, calibrate models
and perform parametric sensitivity analysis. Ladybug and Honeybee are plug-ins
developed for Rhino Grasshopper [@Roudsari2013]. Ladybug is capable to import
and analyze EnergyPlus standard weather data (EPW) in Grasshopper, while
Honeybee can create, run and visualize the OpenStudio and EnergyPlus simulation
results. Both Ladybug and Honeybee take full advantage of the visual scripting
interfaces provided by Grasshopper [@Tabadkani2019]. Since the primary input
files of EnergyPlus are all ASCII text-based, it is possible to update building
energy models by directly process and manipulate the text files, without taking
into account the complex hierarchical structure in the model components. There
are several applications that take this approach. jEplus [@Yi2020] is a
software written in Java to perform complex parametric analysis on multiple
design parameters. It allows users to describe the parameters and their values
using customized symbols in a graphical user interface, automatically create
parametric models using text-substitution [@Zhang2010a]. Modelkit
[@BigLadderSoftware2020] is a free and open-source framework for parametric
model using EnergyPlus. Compared to jEplus, it is capable of automating the
generation and management of EnergyPlus models via its templates and scripting
tools written in Ruby. MLE+ integrates EnergyPlus and scientific computation
and design capacities of Matlab for controller design and can be used to
implement and simulate advanced control algorithms of building systems
[@Bernal2012; @Zhao2013b]. eppy [@Philip2020] is a library for manipulating
EnergyPlus models programmatically via python programming language. It parses
EnergyPlus IDF files into a python object and provides low-level programmatic
access to EnergyPlus inputs. The tools mentioned above may have overlapping in
features but they are tailored for different purposes and use cases, with the
primary focus on ease the time-consuming and error-prone process of creating
and managing parametric simulations. EpXL [ref] is an EnergyPlus Microsoft
Excel user-interface that enables import/export of IDF data files, parametric
analysis and optimization.

BES, with an iterative nature inside, can produce a large amount of data. A large
part of the time has to be spent on the collection and processing of BES
simulation data. Even some BES tools provide summary reports with various
details, it is still quite common in BES to perform post-processing and apply
customized and more advanced algorithms to the simulation results. However,
fewer efforts have been made in terms of providing a seamless integrated
approach to bridge the gap between building energy model and data-centric
analytics. There is a growing body of scientific literature on the application
of advanced mathematical algorithms for building design [@Kiss2020]. Researches
on parametric building energy simulations that employ data mining algorithms
upon large amounts of simulations have emerged [@BurakGunay2019]. Open-source
programming environments such as R and Python have become widely used research
tools that provide access to many well-documented packages for various data
mining, machine learning and data visualization applications. However, the
output of common BES tools is not always friendly in format for applying these
tools, which highlights the potential areas for improvements in data extraction
and result presentation in a clear and intuitive manner for data analytics.

BES is a complex domain that involves multiple scientific processes and is
often a time-intensive, error-prone and hard-to-reproducible process.
Reproducibility is defined as the ability to recompute data analytic results,
given an observed data set and knowledge of the data analysis pipeline
[@Peng2015]. Reproducible research has received an increasing level of
attention throughout the scientific community and the public at large
[@Boettiger2015]. According to a Nature's survey of 1576 researchers in 2016,
more than 70% of them failed to reproduce another scientist's experiments, and
more than 50% failed to reproduce their own experiments. Moreover, more than
half of them agreed that there was a significant crisis of reproducibility
[@Baker2016a].

As BES becomes more and more integral to many aspects of architecture design,
and decision-making processes, computational reproducibility has become an issue of
increasing importance to researchers, designers and practitioners. Lack of
credibility in BES results due to a lack quality and reproducibility is widely
considered a problem by the energy model community [@Fleming2012].

Despite the culture of reluctance and a lack of requirements or incentives to
publish the code used in generating the results, the reasons why BES is usually
not easy to reproduce is mainly caused by 2 aspects:

(a) A lack of seamless integration of BES and data-driven analytics workflows

When conducting BES, most users prefer to use GUI applications which makes it
intuitive and easy to execute certain particular tasks. However, GUI tools have
constraints on flexibility as the users have to specify exactly what and how
features of the design can be manipulated and often are not be able to provide
a good workflow for repeating that task across a wider range of situations on
different systems. In this case, manual steps have to be performed using other
tools, such as a spreadsheet or command-line tools, which introduces
additional transcription burden and results in a non-reproducible process
[@Macumber2012]. Sometimes, custom solutions have to be created from scratch to
automate part portions of the workflows, which may lead to new inefficiencies
and potential errors.

One of the goals of BES is to turn raw simulation data into understanding,
insight, and knowledge. BES projects often encompass an iterative workflow of
raw design data importing and tidying, parametric input file generation and
modification, simulation result collection, processing and analyses, and
finally, conclusion visualization. Most of the time, data analytics of BES
results is of commensurate importance with the model itself. Currently,
there is no widely adopted solution that is able to integrate all processes
into one single platform.

(b) The computational environment of the covering the whole BES
steps is hard to setup

BES often involves the use of multiple applications, software and platforms.
The indispensable step of all To perform crucial scientific processes such as
replicating the results, extending the approach or testing the conclusions in
other contexts, the indispensable step is to install the software used by the
original researchers, which sometimes can become immensely time-consuming if
not impossible. It is easy to underestimate the significant barriers raised by
a lack of familiar, intuitive, and widely adopted tools for addressing the
challenges of computational reproducibility [@Boettiger2015].

In summary, multiple efforts have been made to improve the wide adoption of BES
among researchers, designers and practitioners. However, an overall holistic
framework to bridge the gap between the building energy simulation and data
science domains is still undefined. There is still a need for an integrated
framework to provide a supportive environment to bring reproducible building
energy modeling.

In this paper, we introduce a new parametric simulation framework called eplusr
[@Jia2020] using the R [@RCoreTeam2019] programming language.
eplusr is different from existing frameworks because of its data-centric design
philosophy. The main goals of this framework is to:

1. provide better and more seamless integration between BES and data-driven
   analytics tools. A unified data interface is developed which makes sure all
   simulation data are always stored in a consistent form that matches the
   semantics of the simulation results that can be easily fed to various data
   mining and machine learning algorithms using existing tools in R.
2. provide a rich-featured library that contains a set of abstractions to help
   modify EnergyPlus simulation inputs in a programmatic way and provides a
   simple yet flexible and extensible parametric simulation prototype which can
   be easily extended to perform sensitivity analysis, model calibration and
   optimization.
3. provide infrastructure to bring portable and reusable computation
   environment for building energy model using, which aims to facilitate
   the reproducibility research in building energy domain.

# Eplusr framework overview

Fig. \@ref(fig:architecture) shows an overview of the eplusr framework. Eplusr contains 2 main parts:

1. An R package called eplusr which integrates whole building simulation
   software EnergyPlus and the data-driven analytics environment powered by R
   programming language.
2. An computation environment powered by Docker that contains all necessary
   tools to facilitate reproducible BES.

```{r architecture, echo = FALSE, eval = TRUE, fig.cap = "Eplusr framework architecture", out.width = "40%"}
knitr::include_graphics(here("figures/overview.png"))
```

## The Structure of eplusr package

The eplsur R package is the core component of the framework which provides a
programmatic interface of modifying EnergyPlus models, data interface to
extract simulation results. The package consist of 5 modules:

1. I/O Processing & Parser
2. Model API (Application Program Interface)
3. Tidy Data Extractor
4. Simulation Manager
5. Parametric Prototype

### Parser and Model API

In eplusr, the I/O processing & parser model aims to read and parse EnergyPlus
models and store the data in a format that is both flexible to modify and
capable to store the hierarchical relationship of various model components.
EnergyPlus uses a text input file format named IDF (Input Data File) as a model
file, and each version of an IDF file has a corresponding version of an IDD
(Input Data Dictionary) file who works as a data format schema. In eplusr,
`Idd` class and `IddObject` class provides parsing and data-storing
functionality of an IDD file. These data will be used for subsequent parsing
and data verification of all IDFs of that version. This makes eplusr be able to
handle various versions of IDF files at the same time easily.

Under the hook, eplusr uses a SQL-like structure to store both IDF and IDD data
in various tables. As shown in Fig. \@ref(fig:data-structure). Data of an IDF
is stored in three tables, i.e. object, value, and reference while data of an
IDD is stored in four tables, i.e. group, class, field and reference. With this
data structure, to modify an EnergyPlus model in eplusr is equal to change the
data in those `Idf` tables accordingly, in the context of specific `Idd` data.
In this sense, all methods in `Idf` and `Idd` classes are just interface
wrappers which expose the modification process in a more user-friendly manner.
All the data are masked from end-users but can be easily extracted when needed.

```{r data-structure, echo = FALSE, eval = TRUE, fig.cap = "Data structure of an Idf and Idd object", out.width = "70%"}
knitr::include_graphics(here("figures/data_structure.png"))
```

For most of the time, users do not need to directly interact with `Idd` and
`IddObject` objects, but instead simply using `Idf` and `IdfObject` objects to
modify an IDF file, which provides flexible yet powerful model modification APIs.

In eplusr, the  uses `Idf` class to present a whole IDF file and `IdfObject` class to
present a single object in IDF. Both `Idf` and `IdfObject` classes contain
around thirty member functions for low-level programmatic access to and
modification of the IDF data. Every modification on the IDF data will be
verified to make sure the result complies with underlying IDD.

`Idf` class provides rich-featured interfaces to modify models via various
scope, including scope of a single object, scope of grouped objects level, and
scope of all objects in a certain class.

```{r idf-set, code.cap = "Example of model API", echo = TRUE}
model$set(
  `Supply Fan 1` = list(Fan_Total_Efficiency = 0.9),

  .("Metal Decking", "Metal Roofing") :=
      list(Roughness = "Smooth", Thickness = 0.003),

  Lights := list(Watts_per_Zone_Floor_Area = 7.0)
)
```

Listing \ref{code:idf-set} demonstrates the flexible value modifying scopes
provided by `Idf$set()` method. `model` here is an `Idf` object. In Line 2, the
total efficiency of a `Fan:VariableVolume` object named `Supply Fan 1` will be
changed to 0.9; In line 4-5, the roughness and thickness of two materials named
`Metal Decking` and `Metal Roofing` will be changed; While in line 7, the
lighting power density of all lights will be changed to 7.0 $\mathrm{W}/\mathrm{m}^2$.

Besides of `Idf$set()`, `Idf$update()` method provides an interface that can
achieve the same results but with a `data.frame` input. `data.frame` is an R
representation of a table where each column contains values of one variable. In
Listing 3, Line 1, 4 and 8 extract object data into a `data.table`s
[@Dowle2019], an extension of `data.frame` extremely optimized for speed.
Line 2, 5, 6 and 9 are used to modify certain field values. Line 11-13 are then
call `Idf$update()` method to update object data.

Users can perform any modifications to that data and then update corresponding
object values by feeding the updated data to `Idf$update()` method.

The weather data format used in EnergyPlus is called EPW (EnergyPlus Weather),
a simple, text-based format with comma-separated data. eplusr introduces the
`Epw` class to directly extract both metadata and core weather data of an EPW
file. Moreover, `Epw` class is capable of creating new and modifying existing
EPW files, which is usually an essential process when doing model parameter
calibration.

### Tidy data extractor

When extracting simulation results, instead of directly reading and
manipulating the CSV and HTML files, eplusr uses EnergyPlus SQL output to
extract results of `Output:Variable`, `Output:Meter*`, and `Output:Table`
objects specified in the IDF. The main benefit of this approach is that it
makes sure eplusr always returns the simulation results in a *Tidy Data* format.

Tidy data format was first proposed by @Wickham2014, which is a standard way of
mapping the meaning of a dataset to its structure. A dataset is messy or tidy
depending on how rows, columns and tables are matched up with observations,
variables and types. In tidy data:

* Each variable forms a column.
* Each observation forms a row.
* Each type of observational unit forms a table.

Tidy data makes it easy for an analyst or a computer to extract needed
variables because it provides a standard way of structuring a dataset. Tidy
data is particularly well suited for vectorized programming languages like R,
because the layout ensures that values of different variables from the same
observation are always paired.

However, EnergyPlus CSV output does not present data in a tidy way. Taking
Table \@ref(tab:mess-csv) an example. We are facing one main problem when
working with this type of EnergyPlus CSV output, i.e. column headers contain
values, not only variable names.

In EnergyPlus CSV output, column headers are composed in format *Key value*
(`MEETING_ROOM_1`) + *Variable name* (`Zone Total Internal Gain Rate`) +
*Units* (`W`) + *Reporting frequency* (`Hourly`). This format often leads to
additional data cleaning efforts. For instance, to get all outputs for variable
`Zone Total Internal Latent Gain Rate`, users have to write their methods of
splitting column headers into different parts or subsetting column using
regular expressions.

When using eplusr, the same results in Table \@ref(tab:mess-csv) are always
presented in a tidy format, as shown in Table \@ref(tab:tidy-csv). It
transforms the original column headers from EnergyPlus CSV outputs into four
separate columns, i.e. `key_value`, `name`, `units` and `reporting_frequency`.
Despite those changes, eplusr will also add an additional column named `case`,
which is by default the IDF file name without extension. `case` column can be
quite useful and serve as an indicator to separate each simulation results when
extracting outputs from multiple parametric simulations.

In summary, giving data in a tidy format has several advantages in R, as it
will make sure the data is always fit for directly handling by the *tidyverse*
[@Wickham2019a] package ecosystem, which is a language for solving data-driven analytics
challenges with R code.

```{r mess-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data-raw/mess.csv")), linesep = "",
    caption = "Example of EnergyPlus CSV output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8) %>%
    column_spec(1, "2.5cm")
```

\setlength{\tabcolsep}{1.0pt}
```{r tidy-csv, echo = FALSE, eval = TRUE}
knitr::kable(data.table::fread(here("data-raw/tidy.csv")), linesep = "", align = "c",
    caption = "Tidy format of EnergyPlus output", format = "latex", booktabs = TRUE) %>%
    kable_styling(font_size = 8) %>%
    column_spec(1, "0.9cm") %>%
    column_spec(2, "2.6cm") %>%
    column_spec(3, "1.5cm") %>%
    column_spec(4, "5.2cm") %>%
    column_spec(5, "0.7cm") %>%
    column_spec(6, "2.9cm")
```

Fig. \@ref(fig:data-interface) shows the schematic diagram of eplusr tidy data
interface. Unlike the CSV format output, EnergyPlus's SQL output stores all
simulation results in various tables. For instance, Table. \@ref(tab:tidy-csv)
will be a result of joint of tables `Time`, `EnvironmentPeriods`,
`ReportDataDictionary` and `ReportData`, where the `Time` table contains the
time of simulation, including month, day, hour, day of the week and etc, the
`EnvironmentPeriods` table contains the names and types of the simulated run
period, the `ReportDataDictionary` table provides the dictionary of reported
variables including their parent model component names, variable names, units
and reported frequency, and the `ReportData` contains the actual values of
reported variables. Eplusr result extraction interface provides the
possibilities to query any of the mentioned information which makes it easy and
quite straightforward to get simulation results of any specified time, units
and variables in a consistent manner.
Moreover, instead of presenting the simulated date and time as strings shown in
Table \@ref(tab:mess-csv), eplusr will calculate a proper year value for each
run period and compose a series of `DateTimeClasses` values in the `datetime`
column. This makes it quite straightforward to apply further time-series
analysis on the simulation results.

```{r data-interface, echo = FALSE, eval = TRUE, fig.cap = "Tidy data interface", out.width = "80%"}
knitr::include_graphics(here("figures/result_extraction_interface.png"))
```

### The parametric prototype

eplusr provides a class `ParametricJob` to do parametric simulations. It is
simple to use but is also quite flexible and extensible. It takes full
advantages of eplusr model editing and result collecting functionalities.
Thus, the `ParametricJob` class is capable of defining various analyses using
any algorithms available in R, including sensitivity analysis, optimizations,
and model calibration. The overall process of the framework is shown in Fig.
\@ref(fig:parametric) and can be summarized as follows:

```{r parametric, echo = FALSE, eval = TRUE, fig.cap = "Workflow of a parametric simulation", out.width = "70%"}
knitr::include_graphics(here("figures/parametric.png"))
```

1. Initialize a `ParametricJob` object. Creating a parametric job in eplusr is
   just a simple call of the constructor `param_job()` function. It takes an
   IDF file as the *seed* and an EPW file as the *weather* to be used, as shown
   in Listing 6.

2. Construct a measure function. Here, the concept of measure in eplusr is
   inspired by "measures" in OpenStudio [@Guglielmetti2011]. Basically, a
   measure is a function that takes an `Idf` object and any other arguments as
   input, and returns a modified `Idf` object as output. This approach makes it
   easy for users to access to all the model-editing API (Application
   Programming Interface) that eplusr provides. Listing 7 shows a simple
   measure example that modifies air change rate (ACH) of infiltration objects
   for all zones.

3. Create parametric models. The method `ParametricJob$apply_measure()` allows
   to apply a measure with specified parameter values to an `Idf` and create
   parametric models for later simulations. How the parameter values will be
   generated is all left to the users, which makes it possible to leverage all
   the statistical methods and libraries that R provides. In Listing 8, we
   create 30 parametric models with ACH changing from 0.1 to 3.0 with a 0.1
   step.

4. Run simulations and collect results. Calling `ParametricJob$run()` method
   will run all parametric simulations in parallel and put each simulation
   outputs in a separate folder. After the simulations complete, all results
   from different models can be retrieved in one step using all the data
   collecting APIs, including `ParametricJob$report_data()`,
   `ParametricJob$tabular_data()`, and `ParametricJob$read_table()`. As
   described above, the tables returned will always be presented in a
   tidy-format, which makes it easy to apply any further analyses in the R
   ecosystem. In Listing 8, we collect the annual total site energy of all
   simulations.

One good example of extending the parametric framework in eplusr is the
epluspar [@Jia2020a] R package, which provides new classes for conducting
parametric analysis on EnergyPlus models, including sensitivity analysis using
Morris method [@Morris1991] and Bayesian calibration using the method proposed
by @Chong2017. All the new classes introduced in epluspar package are based on
the `ParametricJob` class. The main differences lie in applying specific
statistical methods for sampling parameter values when calling
`ParametricJob$apply_measure()`.

## Reproducible building energy performance simulation environment

The goal of reproducible research is to tie specific instructions to data
analysis and experimental data so that scholarship can be recreated, better
understood and verified. With the increase in computational tools comes the
advantage of reproducibility at an extent not previously possible. 

Recently, Docker has risen to prominence for "containerisation", the method of taking
software artefacts and building them in such a way that use becomes
standardized and portable across operating systems, and has shown a promising
scalability opportunity [@Nust2020].

The R community has developed numerous packages to facilitate reproducible
research, covering a wide variety of topics, including  literate programming,
pipeline toolkits, package reproducibility, project workflows, code/data
formatting tools, format convertors, and object caching [@Blischak2020].

The Rocker Project was launched in 2014 as a collaboration to provide
high-quality Docker images containing the R environment, and has seen both
considerable uptake in the R community and substantial development and
evolution [@Boettiger2017]. The eplusr-Docker image [@Jia2020d] is built upon
the `rocker/verse` image. It contains an portable and reusable BES computation
environment powered by the RStudio server R programming environment,
EnergyPlus, eplusr package, together with all necessary packages for R-based
data-driven analytics powered by tidyverse package and literate programming
environment powered by RMarkdown and related packages.

A successful reproducible project consists of two major components [@Peng2015]:

* the raw data from the experiment are available
* and that the statistical code and documentation to reproduce that analysis are also available

In a time where BES and decision-makings based on BES are growing in
complexity, the need for reproducibility is growing significantly. From a BES
perspective, the raw data would be the raw building energy models or raw data
to build the models are available. 

Recently, there have been significant efforts to develop software
infrastructure to reproducibly perform and communicate data analyses, including
RMarkdown [@Xie2015], Jupyter notebook [@Kluyver2016], just to name a few.

The RMarkdown format has been a
widely adopted authoring framework for data science. It can be used to both
save and execute code, and generate high-quality reports that can be shared
with an audience. RMarkdown can be easily adapted to any R-centric workflow and
is a good solution to facilitate reproducibility of BEM reporting, since both
the computing code and narratives are in the same document, and results are
automatically generated from the source code. RMarkdown supports dozens of
static and dynamic/interactive output formats.

### Tidyverse for data-driven analytics

Tidyverse is a collection of R packages that share a high-level design
philosophy and low-level grammar and data structures, so that learning one
package makes it easier to learn the next. The tidyverse encompasses the
repeated tasks at the heart of every data science project: data import,
tidying, manipulation, visualisation, and programming [@Wickham2019a].

### RMarkdown for literate programming

The R package knitr is a general-purpose literate programming engine, with
lightweight API's designed to give users full control of the output without
heavy coding work. Together with rmarkdown package and tinytex package, knitr
is able to compile RMarkdown documents to PDF, and also ensures a LaTeX
document is compiled for the correct number of times to resolve all
cross-references.

# Examples and applications

## Data exploration

```{r read-idf, eval = FALSE}
# load package
library(eplusr)

# use example model and weather file
path_model <- file.path(eplus_config(9.1)$dir, "ExampleFiles/RefBldgMediumOfficeNew2004_Chicago.idf")
path_weather <- file.path(eplus_config(9.1)$dir, "WeatherData/USA_IL_Chicago-OHare.Intl.AP.725300_TMY3.epw")

# read model
idf <- read_idf(path_model)

idf$Output_Variable
```

## Data interface

```{r}
# run simulation
job <- model$run(path_epw)

# get metadata of all reported variable
str(job$report_data_dict())

# get the transformer input electric power at 11 a.m for the first day of
# RunPeriod named `SUMMERDAY`, tag this simulation as case `example`, and
# return all possible output columns.
power <- job$report_data("transformer 1", "transformer input electric power", case = "example",
  all = TRUE, simulation_days = 1, environment_name = "summerday", hour = 11, minute = 0)

# get all report variable with Celsius degree unit
str(job$report_data(job$report_data_dict()[units == "C"]))
```

## Parametric simulation of various infiltration rate on energy consumption

```{r}
# create a ParametricJob object
param <- param_job(path_idf, path_epw)

# create a measure for modifying infiltration rate
set_infil_rate <- function (idf, infil_rate) {
  # validate input value
  # this is optional, as validations will be made when setting values
  stopifnot(is.numeric(infil_rate), infil_rate >= 0)
  if (!idf$is_valid_class("ZoneInfiltration:DesignFlowRate"))
    stop("Input model does not have any object in class `ZoneInfiltration:DesignFlowRate`")
  # get all object IDS
  ids <- idf$object_id("ZoneInfiltration:DesignFlowRate", simplify = TRUE)
  # make a list of new values to set
  new_val <- list(design_flow_rate_calculation_method = "AirChanges/Hour", air_changes_per_hour = infil_rate)
  # create proper format for all objects in that class
  val <- rep(list(new_val), length(ids))
  names(val) <- paste0("..", ids)
  idf$set(val)
  idf
}

# create parametric models
param$apply_measure(set_infil_rate, seq(0, 4, by = 1), .names = NULL)

# run in parallel and collect results
param$run(wait = TRUE)

# see the variations of total energy
tab <- param$tabular_data(
    table_name = "Site and Source Energy",
    column_name = "Total Energy",
    row_name = "Total Site Energy"
)
total_eng <- tab[, list(case, `Total Energy (GJ)` = as.numeric(value))]
```

## Multi-optimization using Genetic Algorithm

```{r}
# specify file paths
path_db_win <- here::here("win.idf")
path_db_wall <- here::here("wall.idf")

# create a GA optimization job
ga <- GAOptimJob$new(path_idf, NULL)

# create a function to modify external walls and windows
use_tech <- function (idf, extwall_index = 1L, win_name = "ori") {
    # use ext wall
    if (extwall_index == 0) return(idf)

    const <- read_idf(path_db_wall)$Construction[[sprintf("concrete wall %i", extwall_index)]]
    if (is.null(const)) stop("Invalid wall index")

    dt <- data.table::rbindlist(c(list(const$to_table()), lapply(const$ref_to_object(), function (x) x$to_table())))
    idf$load(dt)

    id_extwall <- idf$to_table(class = "BuildingSurface:Detailed", wide = TRUE)[
        `Outside Boundary Condition` == "Outdoors" & `Surface Type` == "Wall", id]

    idf$set(.(id_extwall) := list(`Construction Name` = const$name()))

    # use window
    if (name == "ori") return(idf)

    win <- read_idf(path_db_win)$Construction[[win_name]]
    dt <- data.table::rbindlist(c(list(win$to_table()), lapply(win$ref_to_object(), function (x) x$to_table())))
    if (is.null(win)) stop("Invalid window name")

    idf$load(dt)
    idf$set("FenestrationSurface:Detailed" := list(`Construction Name` = name))

    idf
}

# specify how to modify the model
ga$apply_measure(
    measure = use_tech,
    extwall_index = integer_space(0:7),
    win_name = choice_space(c(
        "ori",
        "single clear glazing",
        "double clear 16",
        "double clear 25",
        "double lowe 16",
        "double lowe 25",
        "double lowe 25 both coated",
        "triple glazing system",
        "quadruple glazing system"
    ))
)

# define a function to coil desing cooling load
cal_coilloads <- function (idf) {
    job <- idf$last_job()
    stopifnot(!is.null(job))

    job$read_table("ComponentSizes")[comp_type == "Coil:Cooling:Water" & description == "Design Size Design Coil Load", sum(value)]
}

# specify the objective function
ga$objective(cal_coilloads, .dir = "min")

# specify the selector
ga$selector(survival = ecr::selGreedy)

# specify the maximum generation
ga$terminator(max_gen = 20L)

# run optimization
ga$run(mu = 10, dir = here::here("results"))

# read optimization results
read.csv(here::here("Results-2020-05-15.csv"))
```

## Model calibration using Bayesian theory

The `BayesCalibJob` class in the epluspar package implements Bayesian
calibration algorithms proposed by @Chong2017. The basic workflows of
conducting a Bayesian calibration using epluspar package are:

1. Set input and output variables using `$input()` and `$output()`
   respectively.
1. Add parameters to calibrate using `$param()` or `$apply_measure()`.
1. Check parameter sampled values and generated parametric models using
   `$samples()` and `$models()` respectively.
1. Run EnergyPlus simulations in parallel using `$eplus_run()`.
1. Gather simulated data of input and output parameters using `$data_sim()`.
1. Specify field measured data of input and output parameters using
   `$data_field()`.
1.  Specify field measured data of input and output parameters using `$data_field()`.
1. Specify input data for Stan for Bayesian calibration using `$data_bc()`.
1. Run Bayesian calibration and get predictions via Stan using `$stan_run()`.

Listing \@ref(code:bc)

```{r bc, code.cap = "Code for Bayesian calibration"}
# create a `BayesCalibJob` object:
bc <- bayes_job(path_idf, path_epw)

# setting Input and Output Variables
bc$input("CoolSys1 Chiller 1", paste("chiller evaporator", c("inlet temperature", "outlet temperature", "mass flow rate")), "hourly")
bc$output("CoolSys1 Chiller 1", "chiller electric power", "hourly")

# add Parameters to calibrate
bc$param(
    `CoolSys1 Chiller 1` = list(reference_cop = c(4, 6), reference_capacity = c(2.5e6, 3.0e6)),
    .names = c("cop1", "cap1"), .num_sim = 5
)

# get sample values and parametric models
bc$samples()

# run simulations and gather data
bc$eplus_run(dir = tempdir(), run_period = list("example", 7, 1, 7, 3), echo = false)
bc$data_sim("6 hour")

# specify measured data
# NOTE: for demonstration, we use the seed model to generate fake measured output data.
seed <- bc$seed()$clone()
seed$RunPeriod <- NULL
seed$add(RunPeriod = list("test", 7, 1, 7, 3))
seed$SimulationControl$set(
    `Run Simulation for Sizing Periods` = "No",
    `Run Simulation for Weather File Run Periods` = "Yes"
)
seed$save(tempfile(fileext = ".idf"))
job <- seed$run(bc$weather(), echo = FALSE)
fan_power <- epluspar:::report_dt_aggregate(job$report_data(name = bc$output()$variable_name, all = TRUE), "6 hour")
fan_power <- eplusr:::report_dt_to_wide(fan_power)
fan_power <- fan_power[, -"Date/Time"][
    , lapply(.SD, function (x) x + rnorm(length(x), sd = 0.05 * sd(x)))][
    , lapply(.SD, function (x) {x[x < 0] <- 0; x})
]

# set field data
bc$data_field(fan_power)

# specify input data for stan
str(bc$data_bc())

# run Bayesian calibration using Stan
res <- bc$stan_run(iter = 300, chains = 3)

# check calibrated results
rstan::stan_trace(res$fit)
rstan::stan_hist(res$fit)

# check predicted values
str(res$y_pred)
```

# Conclusion

Building energy simulation (BES) has been widely adopted for investigation of
environmental and energy performance for different design and retrofit
alternatives. The absence of seamless integration of BES and data-centric
analysis raises problems in both the productivity and also the credibility of
BES study. This paper proposed a novel holistic framework called 'eplusr' to
bridge the gap between the building energy simulation and data science domains.

Eplusr differs from existing frameworks by its data-centric design philosophy.
It provides a Tidy data format for BES that matches the semantics of the
simulation results. The result extraction interface provides the possibilities
to query any of the mentioned information which makes it easy and quite
straightforward to get simulation results of any specified time, units and
variables in a consistent manner. The Tidy-formatted results can be easily fed
to various data-centric analytics using existing tools in R.

This framework provides a prototype class to perform parametric simulations. It takes full
advantages of eplusr model editing API and Tidy data extraction functionalities.
It is capable of defining various analyses using
any algorithms available in R. The flexibility and extensibility of the
parametric simulation prototype in this framework is demonstrated by its easy
adoption to perform multi-objective optimization and Bayesian calibration.

The need for reproducibility in BEM is growing significantly together with the
ongoing trend of the increasing complexity of BEM project. The eplusr Docker image
provides a possible solution for this by providing a portable and reusable BES
computation environment containing EnergyPlus, eplusr R package, together with
all necessary packages for data-driven analytics and literate programming
environment.

# Acknowledgements

The authors thank the support from the SinBerBEST program.

# References {#references .unnumbered}
